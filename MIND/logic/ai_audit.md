# AI Logic & Behavior Audit

**Date:** 2025-12-12 (Updated: 2025-12-13)
**Modules Audited:** `src/core/lm_client.py`, `src/core/autogpt.py`, `src/core/tools.py`, `src/api/api.py`
**Status:** ‚úÖ **ALL ISSUES FIXED**

## üö® Critical Findings (P0-P1) ‚Äî RESOLVED

### 1. ‚úÖ FIXED: `run_command` Broken on Windows

**File:** `src/core/tools.py`
**Severity:** P1 (Functional Failure)
**Description:**
The `run_command` tool uses `asyncio.create_subprocess_exec` directly with commands like `dir`, `echo`, `type`.
On Windows, these are **shell built-ins**, not executables. They cannot be executed directly.

- `create_subprocess_exec("dir")` -> `FileNotFoundError`.
- **Impact:** The agent will fail to list directories or echo text if it tries to use these commands, thinking it has the capability.
- **Fix:** Must use `cmd /c` prefix for built-ins or use `shell=True` (carefully) or implement internal logic for these instead of shelling out.

### 2. ü§• LOGIC LIE: Auto-GPT Task Completion Heuristic

**File:** `src/core/autogpt.py` (`_mark_task_progress`)
**Severity:** P2 (Logic Flaw)
**Description:**
Tasks are marked as "Completed" based solely on whether a specific *tool* was used (`write_file` -> task containing "write" is done).

- **Scenario:** Goal "Write a quality report". Agent runs `write_file("report.txt", "garbage")`.
- **Result:** System marks task as DONE.
- **Impact:** Agent acts improperly confident. It does not verify the *quality* or *content* of the result, only the *action*.
- **Fix:** The Agent should explicitly *decide* (via LLM) if a subtask is complete, or utilize a verification step.

### 3. üîÄ RACE CONDITION: Optimistic Model Check

**File:** `src/core/lm_client.py` (`ensure_model_loaded`)
**Severity:** P2 (Potential Instability)
**Description:**
The method uses an optimistic check `if self._current_model == required_model: return True` *outside* the lock.

- If Thread A sees `True`, then Thread B (holding lock) updates `self._current_model = None` (unloading).
- Thread A proceeds to request generation on an unloaded model -> API/CLI Error.
- **Fix:** Remove optimistic check or ensure the critical section (generation) also checks state/holds a reader lock.

## üìâ Lazy Implementations (P2-P3)

### 4. `detect_task_type` Hardcoded Keywords

**File:** `src/core/lm_client.py`
**Description:** Uses specific keywords ("–ø–æ—á–µ–º—É", "why") to trigger Reasoning mode.

- **Gap:** Doesn't capture semantic complexity (e.g., "Calculate the orbit..." might not contain keywords but needs reasoning).
- **Fix:** Use a small classifier call or improved heuristics (e.g., "?" count, length, math symbols).

### 5. `run_command` Whitelist is Overly Restrictive

**File:** `src/core/tools.py`
**Description:** Blocks `python`, `node`.

- **Gap:** A "Senior" agent often needs to run python scripts to solve math/logic problems properly.
- **Fix:** Allow `python` on specific isolated script paths or use a safe sandbox exec (e.g. `python -c` is blocked, but maybe allowing running a specific `.py` file created by the agent is acceptable if sandboxed).

## üîç Data Gaps & UX

### 6. Streaming "Loading" State

**File:** `src/api/api.py`
**Description:**
When models switch, the user might see a spinny or nothing. The API sends a custom SSE event `{'status': 'loading'}`, but does the Frontend handle it?

- **Check:** Ensure UI implementation actually shows "Loading Model X..." to the user.

### 7. Step Granularity

**File:** `src/core/autogpt.py`
**Description:**
If `plan()` fails (JSON error), it falls back to 1 giant step.

- **Impact:** User loses visibility into progress.

## ‚úÖ Recommendations

1. **Fix `run_command`**: specific handling for Windows built-ins.
2. **Improve Agent Loop**: Move completion check to LLM ("Is this task actually done?").
3. **Enhance Routing**: Better logic than keyword matching.
4. **Relax Python Sandbox**: Allow running `.py` files generated by the agent (in valid paths), just block `exec/eval` inside them? (Hard to police inside the file, but at least allow running the interpreter).
