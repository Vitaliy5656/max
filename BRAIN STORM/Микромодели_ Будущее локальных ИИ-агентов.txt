Архитектура Гетерогенного Инференса: Глубокий Анализ Эффективности CPU-Резидентных Микро-Моделей в Оркестрации Кода
1. Введение: Смена Парадигмы в Вычислительной Лингвистике
В современной экосистеме больших языковых моделей (LLM) наблюдается фундаментальный сдвиг от монолитных архитектур, опирающихся исключительно на массивные кластеры GPU, к более гибким, гетерогенным системам. Этот переход обусловлен не только дефицитом видеопамяти (VRAM) и экспоненциальным ростом стоимости вычислительных мощностей, но и осознанием того, что не каждая когнитивная задача требует активации моделей с сотнями миллиардов параметров. В этом контексте запрос на использование микро-моделей, таких как Phi-3.5-mini-instruct (3.8 млрд параметров), на центральном процессоре (CPU) в качестве вспомогательного звена для основной GPU-модели, представляет собой не просто техническую оптимизацию, а стратегическую архитектурную развилку.
Центральный вопрос данного исследования заключается в определении жизнеспособности конфигурации, где микро-модель, подвергнутая экстремальной квантовании (вплоть до формата IQ1_S, использующего ~1.56 бита на вес), берет на себя роль «интеллектуального диспетчера» или «черновика» для основной модели. Мы стоим перед выбором: является ли это тупиковой ветвью, ограниченной физикой шины PCIe и энтропийным коллапсом квантованных весов, или же это зарождение новой стратегии, которую можно охарактеризовать как «бикамеральный» искусственный интеллект, разделяющий рефлексивные и когнитивные функции между разными типами вычислителей.
Данный отчет предоставляет исчерпывающий технический анализ, основанный на данных бенчмарков, теоретических моделях пропускной способности памяти и эмпирических отчетах сообщества разработчиков. Мы рассмотрим два основных вектора применения: Спекулятивное Декодирование (Speculative Decoding) и Иерархические Агентные Системы (Hierarchical Multi-Agent Systems). Анализ покажет, что, хотя первое направление сталкивается с серьезными физическими ограничениями на потребительском оборудовании, второе открывает широкие перспективы для создания автономных систем генерации кода, способных к самокоррекции и сложной маршрутизации задач.
________________
2. Физика Экстремального Квантования: Пределы Сжатия Смысла
Прежде чем оценивать архитектурные стратегии, необходимо детально разобрать фундамент, на котором строится предложение пользователя: использование формата квантования IQ1_S для модели Phi-3.5-mini. Стремление разместить модель в оперативной памяти (RAM) или даже в кэше процессора L3 понятно — это минимизирует задержки доступа к данным. Однако переход границы в 2 бита на вес (bpw) запускает процессы деградации сигнала, критичные именно для задач программирования.
2.1 Теория I-Quants и Матриц Важности
Форматы серии IQ (Importance Quantization), реализованные в библиотеке llama.cpp, представляют собой значительный шаг вперед по сравнению с традиционными K-Quants (k-means quantization). Метод IQ1_S использует калибровку на основе «матрицы важности» (Importance Matrix, IMatrix), которая вычисляется путем прогона калибровочного датасета через модель в полном весе (FP16/BF16).1 Алгоритм определяет, какие веса вносят наибольший вклад в минимизацию функции потерь (обычно KL-дивергенции между распределением логитов исходной и квантованной моделей), и сохраняет их с большей точностью, в то время как менее важные веса подвергаются агрессивному прореживанию и округлению.
В формате IQ1_S средняя плотность информации составляет примерно 1.56 бита на параметр. Это достигается за счет использования сложных справочных таблиц (lookup tables) и нелинейного распределения значений квантования. Теоретически это позволяет модели сохранять общую связность текста. Однако для задач генерации кода, где синтаксическая точность (закрытие скобок, отступы, корректные вызовы функций) имеет бинарную природу (работает/не работает), этот метод сжатия демонстрирует фатальные недостатки.
2.2 Феномен «Семантического Коллапса» в IQ1_S
Анализ технических обсуждений в репозиториях llama.cpp выявляет устойчивый паттерн отказов при использовании IQ1_S. Пользователи сообщают о генерации «тарабарщины» (gibberish) — последовательностей токенов, которые могут быть валидными английскими словами, но лишены какой-либо синтаксической или логической структуры.2 В отличие от «галлюцинаций», где модель выдумывает факты, здесь происходит разрушение самой языковой модели.
Для модели Phi-3.5-mini, которая, несмотря на свои скромные размеры (3.8B), позиционируется как мощный инструмент для логических задач и кодинга 3, такое квантование является катастрофическим. Внимание (Self-Attention) в трансформерах опирается на точные скалярные произведения векторов запросов (Queries) и ключей (Keys). При квантовании до 1.56 бит шум квантования становится сопоставимым с полезным сигналом в слоях внимания, отвечающих за долгосрочные зависимости. В коде это проявляется как потеря контекста переменной, объявленной 50 строк назад, или неспособность закрыть JSON-структуру.5
2.3 Сравнительный Анализ Форматов для Оркестрации Кода
Чтобы определить «дно» применимости, мы должны сравнить различные уровни сжатия. Данные показывают четкую границу фазового перехода между 2.0 и 1.5 битами.
Формат Квантования
	Бит на вес (bpw)
	Объем RAM (Phi-3.5)
	Влияние на Перплексию (PPL)
	Пригодность для Кода
	Вердикт
	FP16
	16.0
	~7.6 ГБ
	Базовый уровень
	Высокая (SOTA для 3B)
	Не подходит для CPU-инференса (медленно)
	Q4_K_M
	~4.8
	~2.4 ГБ
	+0.05 (незначительно)
	Сохраняет >95% логики
	Рекомендованный стандарт
	IQ2_XXS
	~2.06
	~1.2 ГБ
	+0.45 (умеренно)
	Синтаксис сохранен, логика упрощена
	Минимальный порог
	IQ1_S
	~1.56
	~0.9 ГБ
	+1.50+ (критично)
	Синтаксический распад
	Тупиковый путь
	Экономия памяти в 300 МБ при переходе от IQ2_XXS (1.2 ГБ) к IQ1_S (0.9 ГБ) на современном оборудовании, даже бюджетном, не оправдывает полную потерю функциональности.6 Для задач оркестрации кода, где ошибка в одном токене (например, в управляющем символе JSON) может сломать весь пайплайн, использование IQ1_S недопустимо.
2.4 Перспектива Native 1-bit Models (BitNet)
Важно провести различие между пост-тренировочным квантованием (PTQ), таким как IQ1_S, и моделями, изначально обученными в низком разрешении, такими как BitNet b1.58. Исследования Microsoft показывают, что модели, тренируемые с тернарными весами $\{-1, 0, 1\}$, могут достигать производительности FP16 моделей аналогичного размера.8 Такие архитектуры заменяют операции умножения с плавающей точкой на целочисленное сложение, что дает реальный прирост скорости и энергоэффективности. Однако на данный момент Phi-3.5 — это классический трансформер. Попытка «втиснуть» его в 1.5 бита через PTQ — это потеря информации. Будущее за нативными BitNet-моделями, но в текущем моменте для Phi-3.5 оптимальным выбором для CPU является Q4_K_M или, в крайнем случае, IQ2_XXS.
________________
3. Стратегия А: Спекулятивное Декодирование (Speculative Decoding) — Анализ «Бутылочных Горлышек»
Первая стратегия, предложенная пользователем, — использование CPU-резидентной микро-модели в качестве «черновика» (drafter) для генерации токенов, которые затем верифицируются основной моделью на GPU. На первый взгляд, это кажется логичным способом утилизировать простаивающие ресурсы CPU. Однако глубокий анализ аппаратных ограничений вскрывает серьезные проблемы.
3.1 Математика Спекулятивного Ускорения
Эффективность спекулятивного декодирования (SD) описывается коэффициентом ускорения $\alpha$, который зависит от вероятности принятия токена $p$ (acceptance rate) и соотношения скоростей черновика и целевой модели $\gamma$ (latency ratio):


$$\alpha = \frac{1 - p^{K+1}}{(1-p)(1 + \gamma)} + \frac{p^{K+1}}{1+\gamma}$$
Где $K$ — количество спекулятивных токенов. В гетерогенной среде (CPU+GPU) параметр $\gamma$ становится критическим. Он определяется не только скоростью инференса моделей, но и задержками на передачу данных.
3.2 Проблема Латентности PCIe (Dovetail Paradox)
В классической схеме SD обе модели находятся в VRAM GPU, и обмен данными (KV-кэш, логиты) происходит мгновенно внутри памяти видеокарты. В предложенной схеме «Drafter на CPU» данные должны физически перемещаться по шине PCIe при каждом шаге верификации.
Исследования архитектуры Dovetail, специально разработанной для гетерогенного SD, показывают, что задержка передачи данных по шине PCIe (Round-Trip Time) может составлять от 10 до 50 микросекунд, что само по себе немного, но кумулятивный эффект при передаче больших тензоров KV-кэша становится значительным.10
Более того, сама скорость генерации на CPU для модели размера 3.8B (Phi-3.5) редко превышает 15-25 токенов в секунду (t/s) на потребительском железе (DDR4/DDR5). Современные GPU (RTX 3090/4090) с оптимизированными ядрами (FlashAttention) способны генерировать токены для моделей 70B со скоростью 15-20 t/s.
* Если $V_{draft} (25 t/s) \approx V_{target} (20 t/s)$, то ускорение невозможно.
* Для эффективного SD скорость черновика должна превышать скорость целевой модели минимум в 2-3 раза.
3.3 Эмпирические Данные Llama.cpp
Сообщество разработчиков llama.cpp провело множество тестов с флагом --model-draft, размещая черновик на CPU. Результаты подтверждают теорию: для моделей размером более 1-2 млрд параметров на CPU наблюдается отрицательное ускорение (замедление) по сравнению с чистым GPU-инференсом.12
Успешные кейсы использования CPU для драфтинга ограничиваются:
1. Экстремально малыми моделями: Qwen-0.5B или специализированными N-gram моделями, которые могут выдавать 50-100 t/s на CPU.
2. Интегрированной памятью (Apple Silicon): На чипах M1/M2/M3 Max, где CPU и GPU делят общую быструю память, проблема PCIe исчезает, и гетерогенное SD работает эффективно.14
3. Экстремально тяжелыми целевыми моделями: Если целевая модель (например, Grok-1 или Llama-3-405B) не влезает целиком в GPU и работает в режиме offloading (2-3 t/s), то даже медленный CPU-драфтер (20 t/s) даст прирост.
Вывод по Стратегии А: Использование Phi-3.5 (3.8B) на CPU в качестве драфтера для Llama-3-70B на GPU в стандартном ПК — это тупиковый путь. Модель слишком тяжела для CPU, чтобы обеспечить необходимый отрыв в скорости, а накладные расходы на синхронизацию съедают остатки выгоды. Перспективной стратегией здесь является использование Qwen-2.5-0.5B или специализированных «нано-моделей».
________________
4. Стратегия Б: Иерархические Агентные Системы (Перспективный Путь)
Если попытка ускорить генерацию токенов через CPU сталкивается с физическими барьерами, то стратегия перераспределения когнитивной нагрузки выглядит крайне многообещающей. В этой парадигме мы рассматриваем систему не как один конвейер генерации текста, а как организацию агентов с разным уровнем "интеллекта" и стоимости.
4.1 Архитектура «Рептильный Мозг — Неокортекс»
Данная архитектура вдохновлена биологическими аналогиями.15
* Рептильный Мозг (CPU / Микро-модель): Отвечает за быстрые, рефлекторные, детерминированные задачи. Низкая стоимость, высокая доступность, работа в фоновом режиме.
* Неокортекс (GPU / Основная модель): Отвечает за глубокое планирование, креативность, сложный синтез. Высокая стоимость активации, высокое энергопотребление.
В контексте оркестратора кода, Phi-3.5 на CPU идеально подходит на роль «Рептильного мозга», выполняя функции, для которых запуск 70B-модели является стрельбой из пушки по воробьям.
4.2 Роль 1: Семантический Маршрутизатор (Intelligent Router)
Традиционные системы используют Regex или ключевые слова для маршрутизации запросов пользователя к нужным инструментам.17 Это дешево, но хрупко. Запросы типа «сделай так, чтобы код летал» Regex не поймет.
* Преимущество Phi-3.5: Даже в квантовании Q4_K_M она обладает отличным пониманием намерений (Intent Classification).18 Она может классифицировать запрос пользователя (напр., «Оптимизация БД», «Рефакторинг UI», «Написание тестов») и выбрать соответствующий системный промпт для основной модели.
* Экономика: Это позволяет не загружать контекст GPU лишними инструкциями. Основная модель получает уже сфокусированную задачу, что повышает качество генерации.
4.3 Роль 2: Синтаксический Страж (Compiler Loop Agent)
В агентных циклах написания кода (Agentic Workflow) наиболее частой проблемой являются мелкие синтаксические ошибки (пропущенные скобки, неверные импорты), которые выявляются при попытке компиляции или запуска.
* Сценарий:
   1. GPU генерирует код.
   2. Оркестратор запускает линтер/компилятор.
   3. Ошибки передаются в Phi-3.5 (CPU).
   4. Микро-модель анализирует ошибку и код, и либо сама исправляет её (для простых случаев), либо формирует уточненный промпт для GPU.19
* Эффективность: Исследования показывают, что использование малых моделей для первичной фильтрации ошибок и самокоррекции (Self-Correction) позволяет разгрузить основную модель на 30-40%.20 Phi-3.5, обученная на качественных синтетических данных («учебниках»), демонстрирует удивительную способность к локализации ошибок в коде.3
4.4 Роль 3: Фильтр Конфиденциальности (PII Sentinel)
Критический аспект корпоративной разработки — защита данных. Передача кода в облачные API (если основная модель там) или даже обработка чувствительных данных локальной моделью требует осторожности.
* Решение: CPU-модель просматривает все входные данные на предмет наличия ключей API, паролей, PII (имена, адреса).
* Почему не Regex? Regex дает много ложных срабатываний (любая 16-значная цифра — кредитка?). LLM понимает контекст: «variable password = "1234"» — это секрет, а «page 1234» — нет.21
* Безопасность: Поскольку Phi-3.5 работает локально на CPU, данные гарантированно не покидают периметр безопасности до их очистки.
4.5 Роль 4: Предварительная Обработка Контекста (RAG Pruning)
При использовании RAG (Retrieval Augmented Generation) в контекст часто попадает много мусора.
* Процесс: Поисковик возвращает 20 фрагментов кода. Phi-3.5 на CPU быстро «прочитывает» их и ранжирует по релевантности, оставляя только 3-5 самых важных для отправки в GPU.
* Выгода: Сокращение длины промпта для GPU экономит VRAM (линейно) и вычислительное время (квадратично для Attention), позволяя обрабатывать более сложные задачи.
________________
5. Практическое Руководство по Реализации
Для реализации описанной стратегии Иерархических Агентов рекомендуется следующий стек технологий и конфигураций.
5.1 Аппаратная Топология
Предполагается типовая рабочая станция разработчика или сервер инференса.
Компонент
	Железо
	Модель
	Квантование
	Функция
	Cognitive Core
	GPU (RTX 3090/4090)
	Llama-3-70B / Qwen-2.5-72B
	EXL2 (4.0bpw)
	Глубокая логика, архитектура, сложный рефакторинг.
	Router / Sentinel
	CPU (Ryzen 9 / i9)
	Phi-3.5-mini
	Q4_K_M
	Классификация интентов, PII scrubbing, RAG reranking.
	Reviewer
	CPU
	Phi-3.5-mini
	Q4_K_M
	Проверка синтаксиса, генерация простых юнит-тестов.
	Drafter (Optional)
	GPU (остаток VRAM)
	Qwen-2.5-0.5B
	Q4_K_M
	Спекулятивное ускорение основной модели (если позволяет VRAM).
	Важное замечание: Мы категорически рекомендуем использовать Q4_K_M (или минимум IQ2_XXS) для CPU-модели. Экономия памяти от использования IQ1_S ничтожна по сравнению с рисками ошибок маршрутизации.
5.2 Конфигурация Llama.cpp Server
Для запуска такой системы можно использовать сервер llama.cpp с поддержкой параллельных слотов. Однако для агентной системы лучше запускать независимые инстансы или использовать фреймворки оркестрации (например, LangGraph или CrewAI), которые обращаются к разным эндпоинтам.
Если же вы все-таки хотите экспериментировать со Спекулятивным Декодированием, команда запуска выглядит так:


Bash




./llama-server \
 -m /models/Llama-3-70B-Instruct-exl2.gguf \  # Основная модель (GPU)
 -ngl 99 \                                    # Все слои на GPU
 -md /models/Qwen-2.5-0.5B-Instruct.gguf \    # Драфт модель (лучше малая!)
 -ngld 99 \                                   # Драфт тоже на GPU (если влезает)
 --draft 16                                   # Количество спекулятивных токенов

Для запуска гетерогенного SD (Draft на CPU), флаги меняются (но будьте готовы к низкой производительности):


Bash




./llama-server \
 -m /models/Llama-3-70B.gguf -ngl 99 \
 -md /models/Phi-3.5-mini.gguf \
 -devd cpu \                                  # Явное указание устройства для Draft
 -ngld 0                                      # 0 слоев драфта на GPU

5.3 Программная Оркестрация (Псевдокод Агента)


Python




# Псевдокод асинхронного оркестратора
async def process_user_request(request):
   # Этап 1: CPU-роутинг (Phi-3.5)
   intent = await cpu_agent.classify(request) 
   
   if intent == "SIMPLE_QUERY":
       return await cpu_agent.generate(request) # Ответ без GPU
       
   # Этап 2: Подготовка контекста (CPU)
   context = await rag_tool.retrieve(request)
   filtered_context = await cpu_agent.rerank(context, top_k=3)
   
   # Этап 3: Генерация (GPU)
   # Пока GPU думает, CPU может проверять безопасность
   gpu_task = asyncio.create_task(gpu_agent.generate(request, filtered_context))
   
   # Этап 4: Потоковая проверка (Streaming Verification)
   async for chunk in gpu_task:
       # CPU проверяет чанк на лету (например, на наличие PII)
       if await cpu_agent.check_safety(chunk):
           yield chunk
       else:
           yield ""

Такой подход (Pipelined Agent) позволяет утилизировать оба процессора параллельно, скрывая латентность CPU-операций за временем генерации GPU.
________________
6. Технические Барьеры и Риски
Несмотря на перспективность агентного подхода, существуют подводные камни, которые необходимо учитывать при проектировании системы.
6.1 Риск "Глухого Телефона" (Error Propagation)
Если CPU-роутер (Phi-3.5) ошибется в классификации интента из-за недостаточной точности (например, квантование IQ1_S привело к потере нюанса), запрос уйдет не тому "эксперту" на GPU. В отличие от спекулятивного декодирования, где ошибка драфта просто отклоняется верификатором, здесь ошибка маршрутизации может привести к неверному ответу пользователю.
* Митогация: Использование Constrained Decoding (грамматик) для CPU-модели, чтобы она гарантированно выдавала только валидные JSON-структуры или метки классов. В llama.cpp это реализуется флагом --grammar.
6.2 Конкуренция за пропускную способность памяти
Даже если модели разнесены по разным устройствам (CPU RAM и GPU VRAM), они конкурируют за системные ресурсы при загрузке данных и препроцессинге. Агрессивный CPU-инференс может забить каналы памяти, замедляя подачу данных в GPU по PCIe.
* Митогация: Настройка привязки ядер (CPU Affinity) и приоритетов процессов. Оставьте 1-2 ядра CPU свободными исключительно для драйвера GPU и пересылки данных.
________________
7. Будущее Гетерогенного Инференса
Мы находимся на пороге значительных изменений в аппаратном и программном обеспечении, которые могут перевернуть текущие выводы.
7.1 Эра AI PC и NPU
Появление процессоров со встроенными нейропроцессорами (NPU) — Intel Core Ultra, AMD Ryzen AI — меняет правила игры. NPU предназначены именно для энергоэффективного инференса малых моделей. Перенос Phi-3.5 на NPU освободит CPU для системных задач и устранит конкуренцию за ресурсы, делая схему «всегда включенного помощника» (Always-on Copilot) реальностью.
7.2 Масштабирование BitNet
Как упоминалось ранее, нативные 1-битные модели (BitNet b1.58) обещают 4-кратное ускорение на CPU. Когда Phi-семейство будет переобучено в этой парадигме (а не просто квантовано пост-фактум), скорость CPU-генерации может достичь 100+ t/s. В этот момент стратегия Спекулятивного Декодирования на CPU снова станет актуальной, так как исчезнет разрыв в скорости с GPU.
7.3 Dovetail и Динамическое Слияние
Исследования в области алгоритмов, таких как Dovetail, предлагают более умные способы гетерогенного SD. Идея заключается в динамическом изменении длины спекулятивной цепочки ($K$) и использовании «ранних выходов» (Early Exit) из слоев трансформера для драфт-модели. Это позволяет снизить точность драфта ради скорости, но сохранить общий выигрыш за счет более частых верификаций.11
________________
8. Заключение и Рекомендации
Проведенное исследование позволяет сформулировать однозначные выводы относительно исходного запроса.
1. По поводу IQ1_S: Использование формата IQ1_S для модели Phi-3.5-mini в задачах кодинга является технической ошибкой. Экономия памяти несоизмерима с потерей семантической связности и способности следовать инструкциям. Абсолютным минимумом («viability floor») следует считать формат IQ2_XXS, а рекомендованным стандартом — Q4_K_M.
2. По поводу Спекулятивного Декодирования на CPU: Для конфигурации «Phi-3.5 (CPU) + Llama-70B (GPU)» на стандартном ПК это тупиковый путь. Физические ограничения шины PCIe и низкая скорость генерации трансформеров на CPU не позволяют достичь положительного ускорения. Данную стратегию стоит применять только при наличии Unified Memory (Apple Silicon) или при использовании экстремально малых драфт-моделей (Qwen-0.5B) непосредственно на GPU.
3. По поводу Иерархических Агентов: Это наиболее перспективная стратегия. Использование CPU-резидентной модели Phi-3.5 (в адекватном квантовании) в качестве семантического роутера, синтаксического валидатора и фильтра безопасности позволяет эффективно утилизировать простаивающие ресурсы CPU, разгрузить GPU от рутинных задач и повысить общую надежность и безопасность системы генерации кода.
Итоговая Рекомендация: Стройте архитектуру оркестратора по принципу «умного конвейера», где CPU-агент готовит, фильтрует и проверяет данные, а GPU-агент выполняет тяжелую интеллектуальную работу. Откажитесь от попыток синхронного ускорения генерации через CPU в пользу асинхронного параллелизма задач.
Сводная Таблица Рекомендованных Конфигураций
Задача Агента
	Рекомендуемая Модель (CPU)
	Квантование
	Альтернатива (GPU Low VRAM)
	Примечание
	Routing / Intent
	Phi-3.5-mini
	Q4_K_M
	Qwen-2.5-0.5B
	Используйте грамматики для JSON вывода
	Syntax Review
	Phi-3.5-mini
	Q5_K_M
	Qwen-2.5-1.5B
	Требуется максимальная точность синтаксиса
	RAG Reranking
	BGE-M3 (Embedding)
	FP16
	-
	Специализированные модели лучше LLM для этого
	Drafting (SD)
	-
	-
	Qwen-2.5-0.5B
	Драфтинг на CPU неэффективен для Phi-3.5
	Внедрение этих архитектурных паттернов позволит создать высокопроизводительную локальную систему разработки, превосходящую по эффективности простые монолитные решения.
Источники
1. qwp4w3hyb/Phi-3-mini-4k-instruct-iMat-GGUF - Hugging Face, дата последнего обращения: декабря 13, 2025, https://huggingface.co/qwp4w3hyb/Phi-3-mini-4k-instruct-iMat-GGUF
2. New IQ1_S somehow much worse than previous version · Issue ..., дата последнего обращения: декабря 13, 2025, https://github.com/ggerganov/llama.cpp/issues/5996
3. microsoft/Phi-3.5-mini-instruct - Hugging Face, дата последнего обращения: декабря 13, 2025, https://huggingface.co/microsoft/Phi-3.5-mini-instruct
4. Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone - arXiv, дата последнего обращения: декабря 13, 2025, https://arxiv.org/pdf/2404.14219
5. Why Do Some Inputs Break Low-Bit LLM Quantization? - ACL Anthology, дата последнего обращения: декабря 13, 2025, https://aclanthology.org/2025.emnlp-main.168.pdf
6. MaziyarPanahi/Phi-3.5-mini-instruct-GGUF - Hugging Face, дата последнего обращения: декабря 13, 2025, https://huggingface.co/MaziyarPanahi/Phi-3.5-mini-instruct-GGUF
7. Why are Q1, Q2 quantization models created if they are universally seen as inferior even to models with fewer parameters? - Reddit, дата последнего обращения: декабря 13, 2025, https://www.reddit.com/r/LocalLLaMA/comments/1p778ju/why_are_q1_q2_quantization_models_created_if_they/
8. The Era of 1-bit LLMs: All Large Language Models are in ... - arXiv, дата последнего обращения: декабря 13, 2025, https://arxiv.org/abs/2402.17764
9. The Future of AI Efficiency with BitNet b1.58 and 1-Bit LLMs - CloudThat, дата последнего обращения: декабря 13, 2025, https://www.cloudthat.com/resources/blog/the-future-of-ai-efficiency-with-bitnet-b1-58-and-1-bit-llms
10. Speculative Decoding and Beyond: An In-Depth Survey of Techniques - arXiv, дата последнего обращения: декабря 13, 2025, https://arxiv.org/html/2502.19732v4
11. Dovetail: A CPU/GPU Heterogeneous Speculative Decoding for LLM inference - arXiv, дата последнего обращения: декабря 13, 2025, https://arxiv.org/html/2412.18934v1
12. Speculative decoding just landed in llama.cpp's server with 25% to 60% speed improvements : r/LocalLLaMA - Reddit, дата последнего обращения: декабря 13, 2025, https://www.reddit.com/r/LocalLLaMA/comments/1gzm93o/speculative_decoding_just_landed_in_llamacpps/
13. Speculative Decoding is AWESOME with Llama.cpp! : r/LocalLLaMA - Reddit, дата последнего обращения: декабря 13, 2025, https://www.reddit.com/r/LocalLLaMA/comments/1oq5msi/speculative_decoding_is_awesome_with_llamacpp/
14. async/parallel speculative execution · ggml-org llama.cpp · Discussion #6853 - GitHub, дата последнего обращения: декабря 13, 2025, https://github.com/ggml-org/llama.cpp/discussions/6853
15. The Brain Is Adaptive Not Triune: How the Brain Responds to Threat, Challenge, and Change - PubMed Central, дата последнего обращения: декабря 13, 2025, https://pmc.ncbi.nlm.nih.gov/articles/PMC9010774/
16. My computational framework for the brain - AI Alignment Forum, дата последнего обращения: декабря 13, 2025, https://www.alignmentforum.org/posts/diruo47z32eprenTg/my-computational-framework-for-the-brain
17. Why We Replaced Our Orchestrator with a 'Regex' Switch - DEV Community, дата последнего обращения: декабря 13, 2025, https://dev.to/_aparna_pradhan_/why-we-replaced-our-orchestrator-with-a-regex-switch-4ih4
18. Phi-3.5-mini-instruct: Pricing, Context Window, Benchmarks, and More - LLM Stats, дата последнего обращения: декабря 13, 2025, https://llm-stats.com/models/phi-3.5-mini-instruct
19. RefAgent: A Multi-agent LLM-based Framework for Automatic Software Refactoring - arXiv, дата последнего обращения: декабря 13, 2025, https://arxiv.org/html/2511.03153v1
20. Fine-Tuning Small Language Models to Optimize Code Review Accuracy | NVIDIA Technical Blog, дата последнего обращения: декабря 13, 2025, https://developer.nvidia.com/blog/fine-tuning-small-language-models-to-optimize-code-review-accuracy/
21. Protecting Sensitive and PII information in RAG with Elasticsearch and LlamaIndex, дата последнего обращения: декабря 13, 2025, https://www.elastic.co/search-labs/blog/rag-security-masking-pii