# Research Lab Module ‚Äî Implementation Plan v2.0

> **–¶–µ–ª—å:** –ü—Ä–µ–≤—Ä–∞—Ç–∏—Ç—å MAX –≤ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—è —Å Double-Pass –æ–±—Ä–∞–±–æ—Ç–∫–æ–π, –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π Topic Skills, –∏ –æ—Ç–¥–µ–ª—å–Ω—ã–º UI –¥–ª—è —Ñ–æ–Ω–æ–≤–æ–≥–æ —Ä–µ–∂–∏–º–∞ —Ä–∞–±–æ—Ç—ã.

> [!NOTE]
> **Router Integration (TODO –ø–æ—Å–ª–µ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏):** –≠—Ç–æ—Ç –ø–ª–∞–Ω —Å–æ–∑–¥–∞—ë—Ç skills –≤ `data/research/skills.json`, –Ω–æ Router –¥–æ–ª–∂–µ–Ω –Ω–∞—É—á–∏—Ç—å—Å—è –∏—Ö —á–∏—Ç–∞—Ç—å. –ü–æ—Å–ª–µ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ —ç—Ç–æ–≥–æ –ø–ª–∞–Ω–∞ ‚Äî –¥–æ–±–∞–≤–∏—Ç—å –≤ `semantic_router.py` –∏–ª–∏ `dynamic_persona.py` —á—Ç–µ–Ω–∏–µ skills.json –∏ –∏–Ω—ä–µ–∫—Ü–∏—é "–î–∏–ø–ª–æ–º–æ–≤" –≤ System Prompt –ø—Ä–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π —Ç–µ–º—ã.

---

## üìã Summary of Changes (v2.0)

| –ß—Ç–æ –∏–∑–º–µ–Ω–∏–ª–æ—Å—å | –ë—ã–ª–æ | –°—Ç–∞–ª–æ | –ü–æ—á–µ–º—É |
|----------------|------|-------|--------|
| LLM Pipeline | 1 pass (Distill) | 3 passes (Miner ‚Üí Jeweler ‚Üí Diploma) | –ö–∞—á–µ—Å—Ç–≤–æ > –°–∫–æ—Ä–æ—Å—Ç—å. Dirty data ‚Üí Clean facts ‚Üí Polished KB ‚Üí Skill |
| Miner Output | Free text | JSON Schema | –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤—ã–≤–æ–¥ –¥–ª—è Pass 2, –Ω–µ —Ç–µ—Ä—è–µ–º –¥–∞–Ω–Ω—ã–µ |
| Fact Deduplication | ‚ùå –ù–µ—Ç | ‚úÖ Embedding similarity | 5 —Å—Ç–∞—Ç–µ–π = 5 –æ–¥–∏–Ω–∞–∫–æ–≤—ã—Ö —Ñ–∞–∫—Ç–æ–≤. –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è —ç–∫–æ–Ω–æ–º–∏—Ç storage |
| Skill Generation | ‚ùå –ù–µ—Ç | ‚úÖ Topic Lens | "–î–∏–ø–ª–æ–º" ‚Äî –∫–æ—Ä–æ—Ç–∫–∏–π prompt –¥–µ–ª–∞—é—â–∏–π MAX —ç–∫—Å–ø–µ—Ä—Ç–æ–º –≤ —Ç–µ–º–µ |
| Chunking Threshold | 4000 tokens | 6000 tokens | –ë–∞–ª–∞–Ω—Å: 4k —Å–ª–∏—à–∫–æ–º –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–æ, 8k —Ä–∏—Å–∫–æ–≤–∞–Ω–Ω–æ –¥–ª—è 14B –º–æ–¥–µ–ª–µ–π |
| Trafilatura Timeout | 8 sec | 10 sec | –î–∞—ë–º –±–æ–ª—å—à–µ –≤—Ä–µ–º–µ–Ω–∏ —Å–ª–æ–∂–Ω—ã–º —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º |
| API Endpoints | 7 | 10 | –î–æ–±–∞–≤–ª–µ–Ω—ã: `/topics/{id}`, `/topics/{id}/skill`, `/topics/{id}/refresh` |
| Skills Storage | ‚Äî | `data/research/skills.json` | –û—Ç–¥–µ–ª—å–Ω—ã–π —Ñ–∞–π–ª, –Ω–µ —Å–º–µ—à–∏–≤–∞–µ–º —Å Soul |

---

## User Review Required

> [!IMPORTANT]  
> **Double-Pass = Triple Cost:** –ö–∞–∂–¥–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ —Ç—Ä–µ–±—É–µ—Ç 3 LLM –≤—ã–∑–æ–≤–∞. –ü—Ä–∏ 20 —Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ö = 60 –≤—ã–∑–æ–≤–æ–≤ = ~5-10 –º–∏–Ω—É—Ç –Ω–∞ –ª–æ–∫–∞–ª—å–Ω–æ–º Qwen-14B. –≠—Ç–æ –æ—Å–æ–∑–Ω–∞–Ω–Ω—ã–π trade-off: –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö –≤–∞–∂–Ω–µ–µ —Å–∫–æ—Ä–æ—Å—Ç–∏.

> [!WARNING]  
> **Trafilatura + Windows:** `pip install trafilatura --prefer-binary` –µ—Å–ª–∏ –æ–±—ã—á–Ω—ã–π pip –ø–∞–¥–∞–µ—Ç.

---

## Dependencies

### [NEW] requirements.txt additions

```txt
# Research Lab
chromadb>=0.4.0        # Vector DB (persistent storage)
trafilatura>=1.6.0     # Clean text extraction
```

### Gotchas & Solutions

| –ü—Ä–æ–±–ª–µ–º–∞ | –†–µ—à–µ–Ω–∏–µ |
|----------|---------|
| ChromaDB + Event Loop: `run_until_complete()` crashes –≤ FastAPI | –í—ã—á–∏—Å–ª—è—Ç—å embeddings –°–ù–ê–†–£–ñ–ò, –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å –≥–æ—Ç–æ–≤—ã–µ –≤–µ–∫—Ç–æ—Ä–∞ –≤ `collection.add()` |
| Trafilatura –∑–∞–≤–∏—Å–∞–µ—Ç 30+ —Å–µ–∫ | `asyncio.wait_for(..., timeout=10)` + fallback –Ω–∞ BS4 |
| Context window overflow (50k chars) | Chunking –ø–µ—Ä–µ–¥ Miner: MAX_CHUNK_TOKENS = 6000 |
| Zombie tasks –ø–æ—Å–ª–µ —Ä–µ—Å—Ç–∞—Ä—Ç–∞ | `cleanup_zombies()` –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ —Å–µ—Ä–≤–µ—Ä–∞ |
| DDG rate limit (50 req/min) | `AsyncRateLimiter(requests=40, period=60)` |
| Duplicate facts | `_deduplicate_facts()` —á–µ—Ä–µ–∑ embedding similarity (threshold 0.9) |

---

## Architecture: Double-Pass Pipeline

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        RESEARCH PIPELINE                            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                     ‚îÇ
‚îÇ  1. PLANNING          2. HUNTING           3. PROCESSING           ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
‚îÇ  ‚îÇ LLM:     ‚îÇ        ‚îÇ DDG      ‚îÇ         ‚îÇ Pass 1: MINER    ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ Generate ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ Search   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ Raw ‚Üí JSON Facts ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ Queries  ‚îÇ        ‚îÇ + Parse  ‚îÇ         ‚îÇ (entities, dates)‚îÇ     ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
‚îÇ                                                    ‚îÇ                ‚îÇ
‚îÇ                                                    ‚ñº                ‚îÇ
‚îÇ                                           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
‚îÇ                                           ‚îÇ Deduplicate      ‚îÇ     ‚îÇ
‚îÇ                                           ‚îÇ (cosine > 0.9)   ‚îÇ     ‚îÇ
‚îÇ                                           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
‚îÇ                                                    ‚îÇ                ‚îÇ
‚îÇ                                                    ‚ñº                ‚îÇ
‚îÇ  4. STORAGE           5. SKILL GEN        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ Pass 2: JEWELER  ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ ChromaDB ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ Pass 3:  ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ Facts ‚Üí KB Entry ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ + skills ‚îÇ        ‚îÇ DIPLOMA  ‚îÇ         ‚îÇ (academic style) ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ   .json  ‚îÇ        ‚îÇ ‚Üí Skill  ‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                   ‚îÇ
‚îÇ                                                                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Proposed Changes

### Backend - Core Module

---

#### [NEW] [src/core/research/**init**.py](file:///c:/Users/Vitaliy/Desktop/MAX/src/core/research/__init__.py)

```python
from .storage import ResearchStorage, research_storage
from .agent import ResearchAgent, research_agent
from .worker import ResearchWorker, research_worker
from .parser import DualParser

__all__ = [
    "ResearchStorage", "research_storage",
    "ResearchAgent", "research_agent", 
    "ResearchWorker", "research_worker",
    "DualParser"
]
```

---

#### [NEW] [src/core/research/storage.py](file:///c:/Users/Vitaliy/Desktop/MAX/src/core/research/storage.py)

ChromaDB wrapper —Å async-safe embedding.

```python
import chromadb
import json
from pathlib import Path
from uuid import uuid4
from typing import Optional
from dataclasses import dataclass

@dataclass
class TopicInfo:
    id: str
    name: str
    description: str
    chunk_count: int
    skill: Optional[str]
    status: str  # LOGIC FIX: "incomplete" | "complete"
    created_at: str

class ResearchStorage:
    """
    ChromaDB-based vector storage for research data.
    
    CRITICAL: Do NOT pass embedding function to Chroma.
    Compute embeddings externally and pass ready vectors.
    """
    
    def __init__(self, persist_dir: str = "data/chroma"):
        self._persist_dir = Path(persist_dir)
        self._client: Optional[chromadb.PersistentClient] = None
        self._embedding_service = None
        self._skills_file = Path("data/research/skills.json")
    
    async def initialize(self, embedding_service):
        """Setup ChromaDB client."""
        self._persist_dir.mkdir(parents=True, exist_ok=True)
        self._client = chromadb.PersistentClient(path=str(self._persist_dir))
        self._embedding_service = embedding_service
        
        # Ensure skills file exists
        if not self._skills_file.exists():
            self._skills_file.parent.mkdir(parents=True, exist_ok=True)
            self._skills_file.write_text("{}")
    
    async def create_topic(self, name: str, description: str = "", status: str = "incomplete") -> str:
        """
        Create new research topic (collection). Returns topic_id.
        
        LOGIC FIX: Added 'status' field. Topics start as 'incomplete'
        and are marked 'complete' only after successful research.
        """
        topic_id = str(uuid4())
        # ChromaDB collection names must be alphanumeric
        safe_name = f"topic_{topic_id.replace('-', '_')}"
        self._client.create_collection(name=safe_name)
        
        # Store metadata with status
        self._save_skill(topic_id, {
            "name": name,
            "description": description,
            "skill": None,
            "status": status,  # LOGIC FIX: Track completion status
            "created_at": datetime.now().isoformat()
        })
        return topic_id
    
    async def update_topic_status(self, topic_id: str, status: str):
        """
        Update topic status.
        LOGIC FIX: Called after successful research or on failure.
        """
        skills = json.loads(self._skills_file.read_text())
        if topic_id in skills:
            skills[topic_id]["status"] = status
            self._skills_file.write_text(json.dumps(skills, indent=2))
    
    async def add_chunk(self, topic_id: str, content: str, metadata: dict):
        """
        Add distilled content to topic collection.
        
        IMPORTANT: Embedding computed OUTSIDE, passed as ready vector.
        This avoids run_until_complete() crash in FastAPI event loop.
        """
        collection = self._get_collection(topic_id)
        
        # Compute embedding EXTERNALLY (async-safe)
        embedding = await self._embedding_service.get_or_compute(content)
        
        if embedding:
            collection.add(
                documents=[content],
                embeddings=[embedding],
                metadatas=[metadata],
                ids=[str(uuid4())]
            )
    
    async def search(self, topic_id: str, query: str, top_k: int = 5) -> list[dict]:
        """Semantic search within topic."""
        collection = self._get_collection(topic_id)
        query_embedding = await self._embedding_service.get_or_compute(query)
        
        if not query_embedding:
            return []
        
        results = collection.query(
            query_embeddings=[query_embedding],
            n_results=top_k
        )
        
        return self._format_results(results)
    
    async def save_skill(self, topic_id: str, skill_prompt: str):
        """Save generated Topic Lens (skill) for a topic."""
        skills = json.loads(self._skills_file.read_text())
        if topic_id in skills:
            skills[topic_id]["skill"] = skill_prompt
            self._skills_file.write_text(json.dumps(skills, indent=2))
    
    async def get_skill(self, topic_id: str) -> Optional[str]:
        """Get skill prompt for topic."""
        skills = json.loads(self._skills_file.read_text())
        return skills.get(topic_id, {}).get("skill")
    
    async def list_topics(self) -> list[TopicInfo]:
        """List all research topics with stats."""
        skills = json.loads(self._skills_file.read_text())
        result = []
        for topic_id, data in skills.items():
            collection = self._get_collection(topic_id)
            result.append(TopicInfo(
                id=topic_id,
                name=data.get("name", "Unknown"),
                description=data.get("description", ""),
                chunk_count=collection.count() if collection else 0,
                skill=data.get("skill"),
                created_at=data.get("created_at", "")
            ))
        return result
```

---

#### [NEW] [src/core/research/parser.py](file:///c:/Users/Vitaliy/Desktop/MAX/src/core/research/parser.py)

Dual parser —Å —Ç–∞–π–º–∞—É—Ç–∞–º–∏ –∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏.

```python
import asyncio
import random
import time
import json
from pathlib import Path
from dataclasses import dataclass
from bs4 import BeautifulSoup

# Lazy import to avoid 5-sec startup delay
trafilatura = None

TRAFILATURA_TIMEOUT = 10  # seconds
MIN_CONTENT_LENGTH = 200  # chars - skip garbage pages

USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/120.0.0.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 14_0) AppleWebKit/605.1.15 Safari/17.0",
    "Mozilla/5.0 (X11; Linux x86_64; rv:120.0) Gecko/20100101 Firefox/120.0",
]

@dataclass
class ParseResult:
    content: str
    parser_used: str  # "trafilatura" | "bs4" | "skipped"
    char_count: int
    parse_time_ms: float

class DualParser:
    """
    Dual parser with trafilatura primary, BS4 fallback.
    
    Features:
    - Timeout protection (10 sec max)
    - A/B metrics collection
    - Garbage filter (skip short pages)
    - Random User-Agent rotation
    """
    
    def __init__(self):
        self._stats = {
            "trafilatura": {"success": 0, "fail": 0, "total_chars": 0, "total_time_ms": 0},
            "bs4": {"success": 0, "fail": 0, "total_chars": 0, "total_time_ms": 0}
        }
        self._stats_file = Path("data/research/parser_stats.json")
        self._load_stats()
    
    async def extract(self, url: str, html: str) -> ParseResult:
        """
        Extract clean text from HTML.
        
        1. Try trafilatura with timeout
        2. Fallback to BS4 if timeout/error
        3. Skip if content too short (garbage page)
        """
        start = time.time()
        
        # Try trafilatura first (with timeout protection)
        content = await self._try_trafilatura(html)
        
        if content and len(content) >= MIN_CONTENT_LENGTH:
            elapsed_ms = (time.time() - start) * 1000
            self._record_stat("trafilatura", True, len(content), elapsed_ms)
            return ParseResult(
                content=content,
                parser_used="trafilatura",
                char_count=len(content),
                parse_time_ms=elapsed_ms
            )
        
        # Fallback to BS4
        content = self._bs4_extract(html)
        elapsed_ms = (time.time() - start) * 1000
        
        if len(content) < MIN_CONTENT_LENGTH:
            self._record_stat("bs4", False, 0, elapsed_ms)
            return ParseResult(
                content="",
                parser_used="skipped",
                char_count=0,
                parse_time_ms=elapsed_ms
            )
        
        self._record_stat("bs4", True, len(content), elapsed_ms)
        return ParseResult(
            content=content,
            parser_used="bs4",
            char_count=len(content),
            parse_time_ms=elapsed_ms
        )
    
    async def _try_trafilatura(self, html: str) -> Optional[str]:
        """Try trafilatura extraction with timeout."""
        global trafilatura
        if trafilatura is None:
            import trafilatura as tf
            trafilatura = tf
        
        try:
            loop = asyncio.get_event_loop()
            content = await asyncio.wait_for(
                loop.run_in_executor(None, trafilatura.extract, html),
                timeout=TRAFILATURA_TIMEOUT
            )
            return content
        except asyncio.TimeoutError:
            from ..logger import log
            log.warn(f"Trafilatura timeout ({TRAFILATURA_TIMEOUT}s), falling back to BS4")
            self._record_stat("trafilatura", False, 0, TRAFILATURA_TIMEOUT * 1000)
            return None
        except Exception:
            return None
    
    def _bs4_extract(self, html: str) -> str:
        """Fallback extraction using BeautifulSoup."""
        soup = BeautifulSoup(html, "html.parser")
        
        # Remove noise
        for tag in soup(["script", "style", "nav", "footer", "header", "aside"]):
            tag.decompose()
        
        text = soup.get_text(separator="\n", strip=True)
        lines = [line.strip() for line in text.splitlines() if line.strip()]
        return "\n".join(lines)
    
    @staticmethod
    def get_random_ua() -> str:
        """Get random User-Agent for requests."""
        return random.choice(USER_AGENTS)
```

---

#### [NEW] [src/core/research/agent.py](file:///c:/Users/Vitaliy/Desktop/MAX/src/core/research/agent.py)

–ì–ª–∞–≤–Ω—ã–π –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä —Å Double-Pass pipeline.

```python
import asyncio
import tiktoken
from uuid import uuid4
from typing import Optional
from dataclasses import dataclass

MAX_CHUNK_TOKENS = 6000  # Safe for 14B models with 32k context
SIMILARITY_THRESHOLD = 0.9  # For fact deduplication

# ============= JSON Schemas for Structured Output =============

MINER_SCHEMA = {
    "type": "object",
    "properties": {
        "entities": {"type": "array", "items": {"type": "string"}},
        "dates": {"type": "array", "items": {"type": "string"}},
        "numbers": {"type": "array", "items": {"type": "string"}},
        "claims": {"type": "array", "items": {"type": "string"}}
    },
    "required": ["entities", "dates", "claims"]
}

MINER_PROMPT = """
You are a fact extraction bot. Extract ONLY raw data from this text.
Output JSON with: entities (people, companies, tools), dates, numbers (stats, metrics), claims (key statements).
Ignore ads, navigation, boilerplate. Be exhaustive but precise.

TEXT:
{content}
"""

JEWELER_PROMPT = """
Synthesize these extracted facts into a structured Knowledge Base entry.
Write in academic style. Be dense and precise. Use Markdown formatting.

EXTRACTED FACTS:
{facts}

OUTPUT FORMAT:
## [Topic Title]
### Overview
[1-2 sentences summary]
### Key Facts
- Fact 1
- Fact 2
### Important Dates
- Date: Event
### Notable Figures/Entities
- Entity: Description
"""

DIPLOMA_PROMPT = """
Based on this research summary, generate a SHORT System Prompt Addendum (3-5 sentences).
This will make an AI assistant an expert in this specific topic.

FORMAT:
[EXPERTISE]: One-line description of knowledge area
[KEY FACTS]: 3-5 bullet points of unique insights from this research
[TONE]: How to communicate about this topic (technical/casual/academic)

RESEARCH SUMMARY:
{summary}

Output the addendum in English.
"""

@dataclass
class ResearchTask:
    id: str
    topic: str
    status: str  # "planning" | "hunting" | "mining" | "polishing" | "diploma" | "complete" | "failed"
    progress: int  # 0-100
    current_stage: str
    pages_found: int = 0
    pages_processed: int = 0
    chunks_stored: int = 0
    error: Optional[str] = None

class ResearchAgent:
    """
    Autonomous research agent with Double-Pass processing.
    
    Pipeline:
    1. Planning: Generate search queries
    2. Hunting: DDG search + page fetch
    3. Mining (Pass 1): Raw text ‚Üí JSON facts
    4. Polishing (Pass 2): Facts ‚Üí KB entry
    5. Diploma (Pass 3): Generate Topic Skill
    """
    
    def __init__(self):
        self._parser: DualParser = None
        self._storage: ResearchStorage = None
        self._lm_client = None
        self._web_searcher = None
        self._embedding_service = None
        self._rate_limiter = AsyncRateLimiter(requests=40, period=60)
        self._tokenizer = tiktoken.get_encoding("cl100k_base")
    
    async def initialize(self, storage, lm_client, web_searcher, embedding_service):
        """Initialize with dependencies."""
        self._storage = storage
        self._lm_client = lm_client
        self._web_searcher = web_searcher
        self._embedding_service = embedding_service
        self._parser = DualParser()
    
    async def research(self, topic: str, description: str = "", max_pages: int = 20) -> str:
        """
        Execute full research pipeline.
        Returns topic_id.
        
        LOGIC FIX: Topic created with status="incomplete".
        Only marked "complete" after all phases succeed.
        """
        # Create topic with "incomplete" status (will be updated at end)
        topic_id = await self._storage.create_topic(topic, description, status="incomplete")
        
        try:
            # Phase 1: Planning
            queries = await self._generate_queries(topic, description)
            
            # Phase 2: Hunting
            all_facts = []
            sources = []  # LOGIC FIX: Track source URLs
            stats = {     # LOGIC FIX: Track research statistics
                "pages_found": 0,
                "pages_processed": 0,
                "pages_skipped": 0,
                "parser_trafilatura": 0,
                "parser_bs4": 0
            }
            
            for query in queries:
                if stats["pages_processed"] >= max_pages:
                    break
                
                await self._rate_limiter.acquire()
                results = await self._web_searcher.search(query, max_results=5)
                stats["pages_found"] += len(results)
                
                for result in results:
                    if stats["pages_processed"] >= max_pages:
                        break
                    
                    # Fetch and parse
                    html = await self._fetch_page(result.url)
                    if not html:
                        stats["pages_skipped"] += 1
                        continue
                    
                    parsed = await self._parser.extract(result.url, html)
                    if parsed.parser_used == "skipped":
                        stats["pages_skipped"] += 1
                        continue
                    
                    # Track parser usage
                    if parsed.parser_used == "trafilatura":
                        stats["parser_trafilatura"] += 1
                    else:
                        stats["parser_bs4"] += 1
                    
                    sources.append(result.url)  # LOGIC FIX: Save source
                    
                    # Phase 3: Mining (Pass 1)
                    facts = await self._mine(parsed.content)
                    all_facts.extend(facts)
                    stats["pages_processed"] += 1
            
            # Deduplicate facts
            unique_facts = await self._deduplicate_facts(all_facts)
            stats["facts_raw"] = len(all_facts)
            stats["facts_unique"] = len(unique_facts)
            
            # Phase 4: Polishing (Pass 2)
            kb_entry = await self._polish(unique_facts)
            
            # Store in ChromaDB with sources and stats
            await self._storage.add_chunk(topic_id, kb_entry, {
                "type": "knowledge_base",
                "topic": topic,
                "facts_count": len(unique_facts),
                "sources": sources,  # LOGIC FIX: Include sources
                "stats": stats       # LOGIC FIX: Include stats
            })
            
            # Phase 5: Diploma (Pass 3) - Generate Skill
            skill = await self._generate_skill(kb_entry)
            await self._storage.save_skill(topic_id, skill)
            
            # Mark topic as complete
            await self._storage.update_topic_status(topic_id, "complete")
            
            return topic_id
            
        except asyncio.CancelledError:
            # LOGIC FIX: On cancel, topic remains "incomplete"
            # User can see partial topic in UI and decide to delete or retry
            raise
    
    async def _fetch_page(self, url: str) -> Optional[str]:
        """
        Fetch page HTML content.
        LOGIC FIX: This method was missing in original plan!
        """
        try:
            return await self._web_searcher.read_page(url)
        except Exception:
            return None
    
    async def research_into_existing(self, topic_id: str, topic: str, description: str, max_pages: int):
        """
        Add new research data to an EXISTING topic.
        LOGIC FIX: Used by refresh - doesn't create new topic, just adds chunks.
        Reuses all the same pipeline (hunting, mining, etc).
        """
        # Skip create_topic - use existing topic_id
        # Rest of pipeline is same as research()
        try:
            queries = await self._generate_queries(topic, description)
            
            all_facts = []
            sources = []
            stats = {
                "pages_found": 0, "pages_processed": 0, "pages_skipped": 0,
                "parser_trafilatura": 0, "parser_bs4": 0
            }
            
            for query in queries:
                if stats["pages_processed"] >= max_pages:
                    break
                await self._rate_limiter.acquire()
                results = await self._web_searcher.search(query, max_results=5)
                stats["pages_found"] += len(results)
                
                for result in results:
                    if stats["pages_processed"] >= max_pages:
                        break
                    html = await self._fetch_page(result.url)
                    if not html:
                        stats["pages_skipped"] += 1
                        continue
                    parsed = await self._parser.extract(result.url, html)
                    if parsed.parser_used == "skipped":
                        stats["pages_skipped"] += 1
                        continue
                    if parsed.parser_used == "trafilatura":
                        stats["parser_trafilatura"] += 1
                    else:
                        stats["parser_bs4"] += 1
                    sources.append(result.url)
                    facts = await self._mine(parsed.content)
                    all_facts.extend(facts)
                    stats["pages_processed"] += 1
            
            unique_facts = await self._deduplicate_facts(all_facts)
            stats["facts_raw"] = len(all_facts)
            stats["facts_unique"] = len(unique_facts)
            
            kb_entry = await self._polish(unique_facts)
            
            # Add to EXISTING topic
            await self._storage.add_chunk(topic_id, kb_entry, {
                "type": "knowledge_base_refresh",
                "topic": topic,
                "facts_count": len(unique_facts),
                "sources": sources,
                "stats": stats
            })
            
            # Regenerate skill with new data
            skill = await self._generate_skill(kb_entry)
            await self._storage.save_skill(topic_id, skill)
            
        except asyncio.CancelledError:
            raise
    
    async def _mine(self, content: str) -> list[str]:
        """
        Pass 1: Extract raw facts from content.
        Uses JSON mode for structured output.
        """
        # Chunk if too long
        chunks = self._chunk_by_tokens(content, MAX_CHUNK_TOKENS)
        all_facts = []
        
        for chunk in chunks:
            response = await self._lm_client.chat(
                messages=[{"role": "user", "content": MINER_PROMPT.format(content=chunk)}],
                response_format={"type": "json_schema", "json_schema": {"schema": MINER_SCHEMA}}
            )
            
            try:
                data = json.loads(response)
                all_facts.extend(data.get("entities", []))
                all_facts.extend(data.get("claims", []))
                all_facts.extend([f"Date: {d}" for d in data.get("dates", [])])
                all_facts.extend([f"Number: {n}" for n in data.get("numbers", [])])
            except:
                pass
        
        return all_facts
    
    async def _deduplicate_facts(self, facts: list[str]) -> list[str]:
        """
        Remove semantically duplicate facts using embedding similarity.
        Threshold: 0.9 cosine similarity = duplicate.
        """
        if not facts:
            return []
        
        unique = []
        embeddings = []
        
        for fact in facts:
            emb = await self._embedding_service.get_or_compute(fact)
            if not emb:
                unique.append(fact)
                continue
            
            # Check against existing embeddings
            is_duplicate = False
            for existing_emb in embeddings:
                similarity = self._cosine_similarity(emb, existing_emb)
                if similarity > SIMILARITY_THRESHOLD:
                    is_duplicate = True
                    break
            
            if not is_duplicate:
                unique.append(fact)
                embeddings.append(emb)
        
        return unique
    
    async def _polish(self, facts: list[str]) -> str:
        """
        Pass 2: Transform facts into polished KB entry.
        """
        facts_text = "\n".join([f"- {f}" for f in facts])
        
        response = await self._lm_client.chat(
            messages=[{"role": "user", "content": JEWELER_PROMPT.format(facts=facts_text)}]
        )
        
        return response
    
    async def _generate_skill(self, summary: str) -> str:
        """
        Pass 3: Generate Topic Lens (skill prompt).
        This makes MAX an "expert" in the researched topic.
        """
        response = await self._lm_client.chat(
            messages=[{"role": "user", "content": DIPLOMA_PROMPT.format(summary=summary)}]
        )
        
        return response
    
    def _chunk_by_tokens(self, text: str, max_tokens: int) -> list[str]:
        """Split text into chunks that fit within token limit."""
        tokens = self._tokenizer.encode(text)
        
        if len(tokens) <= max_tokens:
            return [text]
        
        chunks = []
        for i in range(0, len(tokens), max_tokens):
            chunk_tokens = tokens[i:i + max_tokens]
            chunks.append(self._tokenizer.decode(chunk_tokens))
        
        return chunks
    
    @staticmethod
    def _cosine_similarity(a: list[float], b: list[float]) -> float:
        """Compute cosine similarity between two vectors."""
        dot = sum(x * y for x, y in zip(a, b))
        norm_a = sum(x * x for x in a) ** 0.5
        norm_b = sum(x * x for x in b) ** 0.5
        return dot / (norm_a * norm_b) if norm_a and norm_b else 0.0


class AsyncRateLimiter:
    """Rate limiter for DDG API (40 req/min to be safe)."""
    
    def __init__(self, requests: int = 40, period: int = 60):
        self._semaphore = asyncio.Semaphore(requests)
        self._period = period
    
    async def acquire(self):
        await self._semaphore.acquire()
        asyncio.create_task(self._release_after())
    
    async def _release_after(self):
        await asyncio.sleep(self._period)
        self._semaphore.release()


# Global instance
research_agent = ResearchAgent()
```

---

#### [NEW] [src/core/research/worker.py](file:///c:/Users/Vitaliy/Desktop/MAX/src/core/research/worker.py)

Background worker —Å zombie cleanup.

```python
import asyncio
import json
from pathlib import Path
from uuid import uuid4
from datetime import datetime
from typing import Optional
from fastapi import WebSocket

class ResearchWorker:
    """
    Background research task manager.
    
    Features:
    - Survives client disconnection
    - WebSocket progress updates
    - Zombie cleanup on restart
    - Persistent task state
    """
    
    def __init__(self):
        self._tasks: dict[str, asyncio.Task] = {}
        self._progress: dict[str, dict] = {}
        self._websockets: list[WebSocket] = []
        self._state_file = Path("data/research/active_tasks.json")
        self._log_dir = Path("data/research/logs")
    
    async def cleanup_zombies(self):
        """
        Mark zombie tasks as FAILED on server restart.
        Called from app.py lifespan() before normal init.
        """
        if not self._state_file.exists():
            return
        
        try:
            tasks = json.loads(self._state_file.read_text())
            zombies_found = False
            
            for task_id, task_data in tasks.items():
                if task_data.get("status") == "running":
                    from ..logger import log
                    log.warn(f"Found zombie task {task_id}, marking as FAILED")
                    task_data["status"] = "failed"
                    task_data["error"] = "Server restarted during execution"
                    zombies_found = True
            
            if zombies_found:
                self._state_file.write_text(json.dumps(tasks, indent=2))
        except Exception:
            pass
    
    async def start(self, agent, topic: str, description: str, max_pages: int) -> str:
        """Start research in background task. Returns task_id."""
        
        # LOGIC FIX: Prevent parallel research on same topic
        for tid, progress in self._progress.items():
            if (progress.get("topic") == topic and 
                progress.get("status") == "running"):
                from fastapi import HTTPException
                raise HTTPException(409, f"Research on '{topic}' already in progress")
        
        task_id = str(uuid4())
        
        # Initialize progress
        self._progress[task_id] = {
            "status": "running",
            "topic": topic,  # LOGIC FIX: Track topic name for duplicate check
            "stage": "Planning",
            "percent": 0,
            "started_at": datetime.now().isoformat()
        }
        
        # Save to persistent state
        self._save_state(task_id, self._progress[task_id])
        
        # Create background task
        async def run_research():
            try:
                await self._update_progress(task_id, "Hunting", 10)
                result = await agent.research(topic, description, max_pages)
                await self._update_progress(task_id, "Complete", 100)
                self._progress[task_id]["status"] = "complete"
                self._progress[task_id]["topic_id"] = result
            except Exception as e:
                self._progress[task_id]["status"] = "failed"
                self._progress[task_id]["error"] = str(e)
            finally:
                self._save_state(task_id, self._progress[task_id])
        
        self._tasks[task_id] = asyncio.create_task(run_research())
        return task_id
    
    async def start_refresh(self, agent, existing_topic_id: str, topic: str, description: str, max_pages: int) -> str:
        """
        Start research that adds to EXISTING topic.
        LOGIC FIX: Used by /refresh endpoint to avoid creating duplicates.
        """
        task_id = str(uuid4())
        
        self._progress[task_id] = {
            "status": "running",
            "topic": topic,
            "stage": "Refreshing",
            "percent": 0,
            "started_at": datetime.now().isoformat()
        }
        
        self._save_state(task_id, self._progress[task_id])
        
        async def run_refresh():
            try:
                await self._update_progress(task_id, "Hunting", 10)
                # Use existing topic_id instead of creating new
                await agent.research_into_existing(existing_topic_id, topic, description, max_pages)
                await self._update_progress(task_id, "Complete", 100)
                self._progress[task_id]["status"] = "complete"
                self._progress[task_id]["topic_id"] = existing_topic_id
            except Exception as e:
                self._progress[task_id]["status"] = "failed"
                self._progress[task_id]["error"] = str(e)
            finally:
                self._save_state(task_id, self._progress[task_id])
        
        self._tasks[task_id] = asyncio.create_task(run_refresh())
        return task_id
    
    async def _update_progress(self, task_id: str, stage: str, percent: int, detail: str = ""):
        """
        Update progress and broadcast to WebSockets.
        
        UX REQUIREMENT: Always send human-readable 'detail' field!
        Examples:
        - {"stage": "Hunting", "detail": "Scanning github.com/python/asyncio...", "percent": 10}
        - {"stage": "Mining", "detail": "Extracted 15 facts from article", "percent": 30}
        - {"stage": "M-O Guard", "detail": "Skipped low-quality page (char_count < 200)", "percent": 35}
        """
        self._progress[task_id]["stage"] = stage
        self._progress[task_id]["percent"] = percent
        self._progress[task_id]["detail"] = detail
        
        message = {
            "type": "progress",
            "task_id": task_id,
            "stage": stage,
            "detail": detail,  # Human-readable status!
            "percent": percent
        }
        
        await self._broadcast(message)
    
    async def _broadcast(self, message: dict):
        """Send message to all connected WebSockets."""
        dead = []
        for ws in self._websockets:
            try:
                await ws.send_json(message)
            except:
                dead.append(ws)
        
        for ws in dead:
            self._websockets.remove(ws)
    
    async def get_status(self, task_id: str) -> Optional[dict]:
        """Get current progress of task."""
        return self._progress.get(task_id)
    
    async def cancel(self, task_id: str):
        """Cancel running research."""
        if task_id in self._tasks:
            self._tasks[task_id].cancel()
            self._progress[task_id]["status"] = "cancelled"
            self._save_state(task_id, self._progress[task_id])
    
    async def register_websocket(self, ws: WebSocket):
        """Register WebSocket for progress updates."""
        self._websockets.append(ws)
    
    async def unregister_websocket(self, ws: WebSocket):
        """Remove disconnected WebSocket."""
        if ws in self._websockets:
            self._websockets.remove(ws)
    
    def _save_state(self, task_id: str, data: dict):
        """Persist task state to JSON."""
        self._state_file.parent.mkdir(parents=True, exist_ok=True)
        
        try:
            all_tasks = json.loads(self._state_file.read_text()) if self._state_file.exists() else {}
        except:
            all_tasks = {}
        
        all_tasks[task_id] = data
        self._state_file.write_text(json.dumps(all_tasks, indent=2))


# Global instance
research_worker = ResearchWorker()
```

---

### API Layer

---

#### [NEW] [src/api/routers/research.py](file:///c:/Users/Vitaliy/Desktop/MAX/src/api/routers/research.py)

REST + WebSocket API (10 endpoints).

```python
from fastapi import APIRouter, WebSocket, HTTPException, Query
from pydantic import BaseModel
from typing import Optional

router = APIRouter(prefix="/api/research", tags=["research"])

# ============= Models =============

class StartResearchRequest(BaseModel):
    topic: str
    description: str = ""
    max_pages: int = 20

class TopicInfo(BaseModel):
    id: str
    name: str
    description: str
    chunk_count: int
    skill: Optional[str]
    created_at: str

# ============= Endpoints =============

@router.post("/start")
async def start_research(req: StartResearchRequest) -> dict:
    """Start new research. Returns task_id."""
    from src.core.research import research_agent, research_worker
    task_id = await research_worker.start(
        research_agent,
        topic=req.topic,
        description=req.description,
        max_pages=req.max_pages
    )
    return {"task_id": task_id}

@router.get("/status/{task_id}")
async def get_status(task_id: str) -> dict:
    """Get progress of research task."""
    from src.core.research import research_worker
    status = await research_worker.get_status(task_id)
    if not status:
        raise HTTPException(404, "Task not found")
    return status

@router.post("/cancel/{task_id}")
async def cancel_research(task_id: str):
    """Cancel running research."""
    from src.core.research import research_worker
    await research_worker.cancel(task_id)
    return {"status": "cancelled"}

@router.get("/topics")
async def list_topics() -> list[TopicInfo]:
    """List all research topics with stats."""
    from src.core.research import research_storage
    return await research_storage.list_topics()

@router.get("/topics/{topic_id}")
async def get_topic(topic_id: str) -> TopicInfo:
    """Get details of specific topic."""
    from src.core.research import research_storage
    topics = await research_storage.list_topics()
    for t in topics:
        if t.id == topic_id:
            return t
    raise HTTPException(404, "Topic not found")

@router.get("/topics/{topic_id}/skill")
async def get_topic_skill(topic_id: str) -> dict:
    """Get generated skill prompt for topic."""
    from src.core.research import research_storage
    skill = await research_storage.get_skill(topic_id)
    if not skill:
        raise HTTPException(404, "Skill not found")
    return {"skill": skill}

@router.post("/topics/{topic_id}/refresh")
async def refresh_topic(topic_id: str, max_pages: int = Query(default=10)):
    """
    Re-research a topic with new data.
    
    LOGIC FIX: Adds new chunks to EXISTING topic, not creating duplicate.
    """
    from src.core.research import research_agent, research_storage, research_worker
    
    topics = await research_storage.list_topics()
    topic = next((t for t in topics if t.id == topic_id), None)
    if not topic:
        raise HTTPException(404, "Topic not found")
    
    # LOGIC FIX: Pass existing topic_id to add chunks to same topic
    task_id = await research_worker.start_refresh(
        research_agent,
        existing_topic_id=topic_id,  # Add to existing!
        topic=topic.name,
        description=topic.description,
        max_pages=max_pages
    )
    return {"task_id": task_id}

@router.get("/topics/{topic_id}/search")
async def search_topic(topic_id: str, query: str, top_k: int = 5) -> list[dict]:
    """Semantic search within topic."""
    from src.core.research import research_storage
    return await research_storage.search(topic_id, query, top_k)

@router.delete("/topics/{topic_id}")
async def delete_topic(topic_id: str):
    """Delete topic and all data."""
    from src.core.research import research_storage
    await research_storage.delete_topic(topic_id)
    return {"status": "deleted"}

@router.get("/parser-stats")
async def get_parser_stats() -> dict:
    """Get parser comparison metrics."""
    from src.core.research import research_agent
    return research_agent._parser.get_stats()

# ============= WebSocket =============

@router.websocket("/ws/progress")
async def progress_websocket(ws: WebSocket):
    """Real-time progress updates."""
    from src.core.research import research_worker
    await ws.accept()
    await research_worker.register_websocket(ws)
    try:
        while True:
            await ws.receive_text()
    except:
        pass
    finally:
        await research_worker.unregister_websocket(ws)
```

---

#### [MODIFY] [app.py](file:///c:/Users/Vitaliy/Desktop/MAX/src/api/app.py)

```diff
# Imports
+from src.core.research import research_storage, research_agent, research_worker
+from src.api.routers.research import router as research_router

# In lifespan() - BEFORE other init:
async def lifespan(app: FastAPI):
    ...
+   # Research Lab: Cleanup zombies FIRST
+   await research_worker.cleanup_zombies()
+   
+   # Then initialize normally
+   await research_storage.initialize(embedding_service)
+   await research_agent.initialize(research_storage, lm_client, web_searcher, embedding_service)
+   log.api("üìö Research Lab initialized (ChromaDB + Double-Pass Agent)")
    ...

# Router registration:
+app.include_router(research_router)
```

---

### File Structure Summary

```
src/core/research/           # NEW
‚îú‚îÄ‚îÄ __init__.py              # Exports
‚îú‚îÄ‚îÄ storage.py               # ChromaDB + skills.json
‚îú‚îÄ‚îÄ parser.py                # Dual parser + timeouts + UA rotation
‚îú‚îÄ‚îÄ agent.py                 # Double-Pass pipeline + deduplication
‚îî‚îÄ‚îÄ worker.py                # Background tasks + zombie cleanup

src/api/routers/
‚îî‚îÄ‚îÄ research.py              # 10 REST endpoints + WebSocket

data/research/               # Runtime data
‚îú‚îÄ‚îÄ active_tasks.json        # Task persistence
‚îú‚îÄ‚îÄ parser_stats.json        # Parser metrics
‚îî‚îÄ‚îÄ skills.json              # Topic Skills (Diplomas)

data/chroma/                 # ChromaDB persistence
‚îî‚îÄ‚îÄ [collections]
```

---

## Verification Plan

### Automated Tests

```bash
# New tests
pytest tests/test_research.py -v

# Regression (ensure no breaks)
pytest tests/test_memory.py tests/test_rag.py -v
```

**Test Cases:**

- `test_miner_json_output` ‚Äî verify JSON schema output
- `test_deduplicate_facts` ‚Äî 5 similar facts ‚Üí 1
- `test_diploma_generation` ‚Äî verify skill format
- `test_zombie_cleanup` ‚Äî restart with running task

### Manual Verification

1. `pip install chromadb trafilatura`
2. `python run.py` ‚Äî check for `üìö Research Lab initialized`
3. Start research via API:

   ```bash
   curl -X POST http://localhost:8000/api/research/start \
     -H "Content-Type: application/json" \
     -d '{"topic": "Python async patterns", "max_pages": 5}'
   ```

4. Watch WebSocket progress
5. Verify skill generated: `GET /api/research/topics/{id}/skill`

---

## Rollback Plan

| Problem | Solution |
|---------|----------|
| ChromaDB corrupt | Delete `data/chroma/`, restart |
| Research stuck | Kill process, zombies cleanup on restart |
| Bad skills generated | Edit `skills.json` manually |
| Trafilatura 100% failure | Set `TRAFILATURA_TIMEOUT = 3` to force BS4 fallback |

---

## üé® UI/UX Requirements (Audit Findings)

> [!IMPORTANT]
> –≠—Ç–∏ UX-—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è **–æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã** –¥–ª—è –∫–æ–º—Ñ–æ—Ä—Ç–Ω–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –æ–ø—ã—Ç–∞.

### 1. Visual Stepper (–≠—Ç–∞–ø—ã –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è)

–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –¥–æ–ª–∂–µ–Ω –≤–∏–¥–µ—Ç—å, –Ω–∞ –∫–∞–∫–æ–º —ç—Ç–∞–ø–µ –ø—Ä–æ—Ü–µ—Å—Å:

```
[Planning] ‚Üí [Hunting] ‚Üí [Mining] ‚Üí [Polishing] ‚Üí [Diploma] ‚Üí ‚úì
     ‚óè          ‚óè           ‚óâ            ‚óã            ‚óã
```

```tsx
const STAGES = ["Planning", "Hunting", "Mining", "Polishing", "Diploma"];

<div className="stepper">
  {STAGES.map((s, i) => (
    <div key={s} className={`step ${currentStageIndex > i ? 'done' : currentStageIndex === i ? 'active' : ''}`}>
      {s}
    </div>
  ))}
</div>
```

---

### 2. ETA (Estimated Time)

**Backend:** –î–æ–±–∞–≤–∏—Ç—å `eta_seconds` –≤ WebSocket message:

```python
async def _update_progress(self, task_id: str, stage: str, percent: int, detail: str = ""):
    # Calculate ETA based on progress rate
    elapsed = time.time() - self._progress[task_id]["started_at"]
    if percent > 0:
        eta_seconds = int((elapsed / percent) * (100 - percent))
    else:
        eta_seconds = None
    
    message = {
        "type": "progress",
        "task_id": task_id,
        "stage": stage,
        "detail": detail,
        "percent": percent,
        "eta_seconds": eta_seconds  # NEW
    }
```

**Frontend:**

```tsx
{progress.eta_seconds && (
  <span className="eta">‚è±Ô∏è ~{Math.ceil(progress.eta_seconds / 60)} min remaining</span>
)}
```

---

### 3. "Safe to Close" Message

–ü—Ä–∏ —Å—Ç–∞—Ä—Ç–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø–æ–∫–∞–∑–∞—Ç—å:

```tsx
const [showSafeMessage, setShowSafeMessage] = useState(false);

// On research start success:
setShowSafeMessage(true);
setTimeout(() => setShowSafeMessage(false), 5000);

// Render:
{showSafeMessage && (
  <div className="safe-message">
    ‚úÖ Research is running in background. You can close this tab ‚Äî it will continue working.
  </div>
)}
```

---

### 4. Empty State (First Run)

–ö–æ–≥–¥–∞ —Å–ø–∏—Å–æ–∫ —Ç–µ–º –ø—É—Å—Ç:

```tsx
{topics.length === 0 ? (
  <div className="empty-state">
    <span className="emoji">üìö</span>
    <h3>Your Knowledge Library is empty</h3>
    <p>Start your first research to build MAX expertise in any topic</p>
    <button onClick={() => setShowForm(true)}>+ New Research</button>
  </div>
) : (
  <TopicList topics={topics} />
)}
```

---

### 5. Cancel Confirmation

–ü–µ—Ä–µ–¥ –æ—Ç–º–µ–Ω–æ–π ‚Äî –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ:

```tsx
const handleCancel = (taskId: string) => {
  if (window.confirm("Cancel this research? Progress will be lost.")) {
    cancelResearch(taskId);
  }
};

<button className="cancel-btn" onClick={() => handleCancel(task.id)}>
  ‚úï Stop
</button>
```

---

### 6. Skeleton Loading

–ü–æ–∫–∞ —Å–ø–∏—Å–æ–∫ —Ç–µ–º –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è:

```tsx
{isLoading ? (
  <div className="skeleton-list">
    {[1, 2, 3].map(i => (
      <div key={i} className="skeleton-item">
        <div className="skeleton-title" />
        <div className="skeleton-meta" />
      </div>
    ))}
  </div>
) : (
  <TopicList topics={topics} />
)}
```

CSS:

```css
.skeleton-item {
  animation: pulse 1.5s ease-in-out infinite;
}
@keyframes pulse {
  0%, 100% { opacity: 0.4; }
  50% { opacity: 0.8; }
}
```

---

### 7. Celebration Modal! üéâ

–ü—Ä–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è:

```tsx
{completedTask && (
  <div className="completion-modal">
    <span className="confetti">üéâ</span>
    <h2>Research Complete!</h2>
    <p>MAX is now an expert in <strong>{completedTask.topic}</strong></p>
    <div className="stats">
      <span>{completedTask.facts_count} facts extracted</span>
      <span>{completedTask.sources_count} sources analyzed</span>
    </div>
    <button onClick={() => viewTopic(completedTask.topic_id)}>
      View Knowledge Base
    </button>
    <button className="secondary" onClick={() => setCompletedTask(null)}>
      Close
    </button>
  </div>
)}
```

---

### UX Checklist Summary

| Requirement | Priority | Status |
|-------------|----------|--------|
| Stepper (5 stages) | üî¥ High | TODO |
| ETA countdown | üî¥ High | TODO |
| "Safe to close" toast | üü° Medium | TODO |
| Empty state | üî¥ High | TODO |
| Cancel confirmation | üü° Medium | TODO |
| Skeleton loading | üü° Medium | TODO |
| Celebration modal | üü¢ Nice-to-have | TODO |

---

## üß† Phase 2: Brain Map Visualization

> [!NOTE]
> **–û–¢–î–ï–õ–¨–ù–´–ô –§–ê–ô–õ:** –ü–æ–¥—Ä–æ–±–Ω—ã–π –ø–ª–∞–Ω Brain Map –≤—ã–Ω–µ—Å–µ–Ω –≤:
>
> üìÑ **[Brain_Map_Plan.md](file:///c:/Users/Vitaliy/Desktop/MAX/BRAIN%20STORM/Extended_Search_Library/Brain_Map_Plan.md)**

### Quick Summary

| –ö–æ–º–ø–æ–Ω–µ–Ω—Ç | –†–µ—à–µ–Ω–∏–µ |
|-----------|---------|
| Entry Point | `DenseCore.tsx` ‚Üí `BrainCore.tsx` (–∫–ª–∏–∫–∞–±–µ–ª—å–Ω—ã–π) |
| Backend | UMAP –≤ `run_in_executor` (–Ω–µ –±–ª–æ–∫–∏—Ä—É–µ—Ç) |
| Data Limit | Hierarchical UMAP (3 —É—Ä–æ–≤–Ω—è zoom) |
| Stability | Cached reducer + `.transform()` |
| Loading | 3D Skeleton (ghost spheres) |
| Frontend | `react-three-fiber` + `@react-three/drei` |

### Files

```
src/core/research/brain_map.py     # UMAP logic
src/components/BrainCore.tsx       # Clickable indicator  
src/components/BrainMapModal.tsx   # 3D modal
```

### Code Marker

‚úÖ TODO –¥–æ–±–∞–≤–ª–µ–Ω –≤ `frontend/src/components/DenseCore.tsx`
