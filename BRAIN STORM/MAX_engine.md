# üèóÔ∏è MAX ENGINE: –ê–†–•–ò–¢–ï–ö–¢–£–†–ê (DESIGN DOCUMENT)

**–í–µ—Ä—Å–∏—è:** 1.0
**–¶–µ–ª–µ–≤–æ–µ –∂–µ–ª–µ–∑–æ:** RTX 4070 Ti Super (16GB VRAM)
**–ë–∞–∑–æ–≤–∞—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞:** `ExLlamaV2` (Python/CUDA/C++)

---

## 1. üì¶ –ó–ê–í–ò–°–ò–ú–û–°–¢–ò (DEPENDENCIES)

–ù–µ–æ–±—Ö–æ–¥–∏–º—ã–µ Python –ø–∞–∫–µ—Ç—ã –¥–ª—è –æ–∫—Ä—É–∂–µ–Ω–∏—è.

```txt
# Core Inference (–Ø–¥—Ä–æ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞)
exllamav2>=0.0.12      # –°–∞–º—ã–π –±—ã—Å—Ç—Ä—ã–π –¥–≤–∏–∂–æ–∫ –¥–ª—è Llama/Qwen –Ω–∞ 40xx
torch>=2.2.0           # PyTorch –¥–ª—è —Ç–µ–Ω–∑–æ—Ä–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π
numpy

# API & Server (–°–µ—Ä–≤–µ—Ä–Ω–∞—è —á–∞—Å—Ç—å)
fastapi                # –°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π Async API —Ñ—Ä–µ–π–º–≤–æ—Ä–∫
uvicorn                # ASGI —Å–µ—Ä–≤–µ—Ä –¥–ª—è –∑–∞–ø—É—Å–∫–∞ FastAPI
pydantic               # –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö

# Vector Management (–ë–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤)
sqlite-vec             # –•—Ä–∞–Ω–µ–Ω–∏–µ steering-–≤–µ–∫—Ç–æ—Ä–æ–≤ (–≤–µ–∫—Ç–æ—Ä–æ–≤ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è)
sqlite3                # –î—Ä–∞–π–≤–µ—Ä –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö

# Utilities
websockets             # –î–ª—è real-time —Å—Ç—Ä–∏–º–∏–Ω–≥–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
aiohttp                # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π HTTP –∫–ª–∏–µ–Ω—Ç
```

---

## 2. ‚öôÔ∏è –û–ë–ó–û–† –ê–†–•–ò–¢–ï–ö–¢–£–†–´

–°–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç **Unified-Process** (–ï–¥–∏–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å) –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É. –£—á–∏—Ç—ã–≤–∞—è –∂–µ—Å—Ç–∫–æ–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –≤ 16GB VRAM, —ç—Ç–æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ: –ë–æ–ª—å—à–∞—è –∏ –ú–∞–ª–∞—è –º–æ–¥–µ–ª–∏ –¥–æ–ª–∂–Ω—ã –∂–∏—Ç—å –≤ –æ–¥–Ω–æ–º Python-–ø—Ä–æ—Ü–µ—Å—Å–µ, —á—Ç–æ–±—ã –¥–µ–ª–∏—Ç—å –≤–∏–¥–µ–æ–ø–∞–º—è—Ç—å (Zero-copy) –∏ –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å —Ç–µ–Ω–∑–æ—Ä—ã –±–µ–∑ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è —á–µ—Ä–µ–∑ CPU.

### –†–∞—Å–∫–ª–∞–¥–∫–∞ –ø–∞–º—è—Ç–∏ (–¶–µ–ª—å: 16GB VRAM)

* **Big Model (Qwen-2.5-7B-Instruct-GPTQ-Int4 / EXL2)**: ~5.0 GB
* **Small Model (Qvikhr-1.5B-Instruct-EXL2)**: ~1.5 GB
* **KV Cache (Shared/Paged)**: ~8.0 GB (–î–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ –≤—ã–¥–µ–ª–µ–Ω–∏–µ —á–µ—Ä–µ–∑ Paged Attention)
* **Overhead (–ë—É—Ñ–µ—Ä—ã CUDA, PyTorch)**: ~1.5 GB

### –ö–æ–Ω—Ü–µ–ø—Ü–∏—è "–û–±—â–µ–≥–æ –ö–æ–Ω—Ç–µ–∫—Å—Ç–∞" (Context Mirror)

–¢–∞–∫ –∫–∞–∫ Qwen –∏ Qvikhr –∏–º–µ—é—Ç —Ä–∞–∑–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É (—Ä–∞–∑–º–µ—Ä —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–µ–≤), –æ–Ω–∏ **–Ω–µ –º–æ–≥—É—Ç** —Ñ–∏–∑–∏—á–µ—Å–∫–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ–¥–∏–Ω –∏ —Ç–æ—Ç –∂–µ KV Cache.
**–†–µ—à–µ–Ω–∏–µ:** `ContextMirror` (–ó–µ—Ä–∫–∞–ª–æ –ö–æ–Ω—Ç–µ–∫—Å—Ç–∞). –û—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä —Ö—Ä–∞–Ω–∏—Ç *–ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¢–æ–∫–µ–Ω–æ–≤*. –û–±–µ –º–æ–¥–µ–ª–∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç –æ–¥–Ω–∏ –∏ —Ç–µ –∂–µ –≤—Ö–æ–¥–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã. –ú–∞–ª–∞—è –º–æ–¥–µ–ª—å "–ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ" —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ—Ç—Å—è —Å –ë–æ–ª—å—à–æ–π: –∫–∞–∫ —Ç–æ–ª—å–∫–æ –ë–æ–ª—å—à–∞—è —Ä–æ–∂–¥–∞–µ—Ç —Ç–æ–∫–µ–Ω, –æ–Ω —Å–∫–∞—Ä–º–ª–∏–≤–∞–µ—Ç—Å—è –ú–∞–ª–æ–π.

* **"–í–∑–≥–ª—è–¥ –Ω–∞ –ª–æ–≥–∏—Ç—ã"**: –ú–∞–ª–∞—è –º–æ–¥–µ–ª—å –¥–µ–ª–∞–µ—Ç forward pass (–ø—Ä–æ—Ö–æ–¥) –ø–æ —Ç–µ–∫—É—â–µ–º—É –∫–æ–Ω—Ç–µ–∫—Å—Ç—É –∏ –º–æ–∂–µ—Ç "–ø–æ–¥—Å–º–æ—Ç—Ä–µ—Ç—å" –ª–æ–≥–∏—Ç—ã –ë–æ–ª—å—à–æ–π –º–æ–¥–µ–ª–∏, —á—Ç–æ–±—ã –≤—ã–Ω–µ—Å—Ç–∏ —Å—É–∂–¥–µ–Ω–∏–µ (–æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å? –≤–º–µ—à–∞—Ç—å—Å—è?).

---

## 3. üèóÔ∏è –°–¢–†–£–ö–¢–£–†–ê –ö–õ–ê–°–°–û–í (PYTHON)

### A. `SteeringController` (–ö–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä –≤–º–µ—à–∞—Ç–µ–ª—å—Å—Ç–≤–∞)

–£–ø—Ä–∞–≤–ª—è–µ—Ç "–ù–µ–π—Ä–æ—Ö–∏—Ä—É—Ä–≥–∏–µ–π". –•—Ä–∞–Ω–∏—Ç –≤–µ–∫—Ç–æ—Ä—ã –∏ –æ—Ç–≤–µ—á–∞–µ—Ç –∑–∞ –º–∞—Ç–µ–º–∞—Ç–∏–∫—É –∏–Ω—ä–µ–∫—Ü–∏–π.

```python
import torch
import numpy as np

class SteeringController:
    """
    –£–ø—Ä–∞–≤–ª—è–µ—Ç –≤–µ–∫—Ç–æ—Ä–∞–º–∏ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π (steering vectors) –∏ –∏—Ö –≤–Ω–µ–¥—Ä–µ–Ω–∏–µ–º –≤ –º–æ–¥–µ–ª—å.
    """
    def __init__(self, device="cuda"):
        self.vectors = {}  # {'sadness': torch.Tensor, ...}
        self.active_hooks = []
        self.device = device

    def load_vector(self, name: str, vector_data: np.ndarray):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –≤–µ–∫—Ç–æ—Ä–∞ –∏–∑ –ë–î –≤ VRAM."""
        tensor = torch.from_numpy(vector_data).to(self.device).half()
        self.vectors[name] = tensor

    def get_injection(self, layer_idx: int) -> torch.Tensor | None:
        """–†–∞—Å—á–µ—Ç –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Å–ª–æ—è."""
        # –°—É–º–º–∏—Ä—É–µ–º –∞–∫—Ç–∏–≤–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã –¥–ª—è —ç—Ç–æ–≥–æ —Å–ª–æ—è
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º None, –µ—Å–ª–∏ –≤–º–µ—à–∞—Ç–µ–ª—å—Å—Ç–≤–æ –Ω–µ –Ω—É–∂–Ω–æ
        pass
```

### B. `ExLlamaEngine` (–û–±–µ—Ä—Ç–∫–∞)

–û–±–µ—Ä—Ç–∫–∞ –≤–æ–∫—Ä—É–≥ `ExLlamaV2`, –ø–æ–∑–≤–æ–ª—è—é—â–∞—è –∏—Ç–µ—Ä–∏—Ä–æ–≤–∞—Ç—å—Å—è –ø–æ —Å–ª–æ—è–º –≤—Ä—É—á–Ω—É—é (—á—Ç–æ–±—ã –≤—Å—Ç–∞–≤–ª—è—Ç—å —Ö—É–∫–∏).

```python
from exllamav2 import ExLlamaV2, ExLlamaV2Config, ExLlamaV2Cache, ExLlamaV2Tokenizer

class ExLlamaEngine:
    def __init__(self, model_dir: str, max_seq_len: int = 8192):
        self.config = ExLlamaV2Config()
        self.config.model_dir = model_dir
        self.config.prepare()
        
        self.model = ExLlamaV2(self.config)
        self.cache = ExLlamaV2Cache(self.model, max_seq_len=max_seq_len)
        self.tokenizer = ExLlamaV2Tokenizer(self.config)
        
        self.model.load_autosplit(self.cache)

    def forward_with_steering(self, 
                            input_ids: torch.Tensor, 
                            steering_controller: SteeringController):
        """
        –ö–∞—Å—Ç–æ–º–Ω—ã–π –ø—Ä–æ—Ö–æ–¥ (forward pass), –∫–æ—Ç–æ—Ä—ã–π –∏—Ç–µ—Ä–∏—Ä—É–µ—Ç—Å—è –ø–æ —Å–ª–æ—è–º –¥–ª—è –∏–Ω—ä–µ–∫—Ü–∏–π.
        """
        # –≠—Ç–æ "–•–∏—Ä—É—Ä–≥–∏—á–µ—Å–∫–∏–π" –ø—Ä–æ—Ö–æ–¥
        # 1. –≠–º–±–µ–¥–¥–∏–Ω–≥–∏
        hidden_states = self.model.modules[0].forward(input_ids, self.cache)
        
        # 2. –ò—Ç–µ—Ä–∞—Ü–∏—è –ø–æ —Å–ª–æ—è–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞
        for i, layer in enumerate(self.model.modules[1:], start=1):
            
            # A. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∏–Ω—ä–µ–∫—Ü–∏—é (–•—É–∫)
            injection = steering_controller.get_injection(layer_idx=i)
            if injection is not None:
                hidden_states += injection * 0.5  # –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å–∏–ª—ã
            
            # B. –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø—Ä–æ—Ö–æ–¥ —Å–ª–æ—è
            # –í–∞–∂–Ω–æ: —Å–∏–≥–Ω–∞—Ç—É—Ä–∞ –º–æ–∂–µ—Ç –æ—Ç–ª–∏—á–∞—Ç—å—Å—è –≤ —Ä–∞–∑–Ω—ã—Ö –≤–µ—Ä—Å–∏—è—Ö ExLlamaV2
            hidden_states = layer.forward(hidden_states, self.cache, ...)
            
        return hidden_states
```

### C. `CognitiveOrchestrator` (–î–∏—Ä–∏–∂–µ—Ä)

–ì–ª–∞–≤–Ω—ã–π —Ü–∏–∫–ª –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏. –£–ø—Ä–∞–≤–ª—è–µ—Ç –æ–±–µ–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏.

```python
class CognitiveOrchestrator:
    def __init__(self, big_model: ExLlamaEngine, small_model: ExLlamaEngine):
        self.big_model = big_model
        self.small_model = small_model
        self.steering = SteeringController()

    async def generate_orchestrated(self, prompt: str):
        # 1. Prefill (–∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞) –æ–±–µ–∏—Ö –º–æ–¥–µ–ª–µ–π
        ids = self.big_model.tokenizer.encode(prompt)
        self.big_model.prefill(ids)
        self.small_model.prefill(ids)

        stop_condition = False
        while not stop_condition:
            # 2. –®–∞–≥ –ë–æ–ª—å—à–æ–π –ú–æ–¥–µ–ª–∏ (–ü–æ–ª—É—á–∞–µ–º –õ–æ–≥–∏—Ç—ã - –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è)
            big_logits = self.big_model.forward_step()
            
            # 3. –°—É–¥ –ú–∞–ª–æ–π –ú–æ–¥–µ–ª–∏
            # –ú–∞–ª–∞—è –º–æ–¥–µ–ª—å —Å–º–æ—Ç—Ä–∏—Ç –Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) –Ω–∞ –ø–ª–∞–Ω—ã –ë–æ–ª—å—à–æ–π
            # decision = self.small_model.judge(big_logits, context)
            
            decision = self.decide_action(big_logits) 
            
            if decision == "STEER":
                # –ü—Ä–∏–º–µ–Ω—è–µ–º –≤–µ–∫—Ç–æ—Ä –∏ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞–µ–º —à–∞–≥ –ë–æ–ª—å—à–æ–π –º–æ–¥–µ–ª–∏
                self.steering.activate("logic_boost", layer=15)
                big_logits = self.big_model.forward_step_with_steering(self.steering)
            
            # 4. –°—ç–º–ø–ª–∏–Ω–≥ (–í—ã–±–æ—Ä —Ç–æ–∫–µ–Ω–∞) –∏ –≤—ã–¥–∞—á–∞
            token = self.sample(big_logits)
            
            # –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ–º KV-–∫—ç—à –ú–∞–ª–æ–π –º–æ–¥–µ–ª–∏ —Å –≤—ã–±—Ä–∞–Ω–Ω—ã–º —Ç–æ–∫–µ–Ω–æ–º
            self.small_model.update_cache(token)
            
            yield token
            
            if token == self.big_model.tokenizer.eos_token_id:
                stop_condition = True
```

---

## 4. üß¨ –ü–†–ò–ú–ï–†: –í–ù–ï–î–†–ï–ù–ò–ï –í–ï–ö–¢–û–†–ê –í EXLLAMAV2

–ü—Ä–∏–º–µ—Ä —Ç–æ–≥–æ, –∫–∞–∫ —Ä–µ–∞–ª—å–Ω–æ –≤–Ω–µ–¥—Ä–∏—Ç—å –≤–µ–∫—Ç–æ—Ä –≤ `ExLlamaV2`, –æ–±—Ö–æ–¥—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ fused-—è–¥—Ä–∞ –∏ –∑–∞–ø—É—Å–∫–∞—è —Å–ª–æ–∏ –≤—Ä—É—á–Ω—É—é. Python-—Ü–∏–∫–ª —Ä–∞–±–æ—Ç–∞–µ—Ç –º–µ–¥–ª–µ–Ω–Ω–µ–µ (–Ω–∞ 10-15%), –Ω–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –¥–µ–ª–∞—Ç—å "–º–∞–≥–∏—é".

```python
import torch
from exllamav2 import ExLlamaV2, ExLlamaV2Config, ExLlamaV2Cache

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
config = ExLlamaV2Config()
config.model_dir = "models/Qwen2.5-7B-Instruct-exl2"
config.prepare()

model = ExLlamaV2(config)
cache = ExLlamaV2Cache(model, lazy=True)
model.load_autosplit(cache)

# --- –î–ê–ù–ù–´–ï –î–õ–Ø –í–ú–ï–®–ê–¢–ï–õ–¨–°–¢–í–ê ---
# –†–∞–∑–º–µ—Ä –≤–µ–∫—Ç–æ—Ä–∞ –¥–æ–ª–∂–µ–Ω —Å–æ–≤–ø–∞–¥–∞—Ç—å —Å hidden_size (–Ω–∞–ø—Ä., 4096 –¥–ª—è 7B)
steering_vector = torch.load("vectors/creativity_v1.pt").to("cuda").half()
target_layer_idx = 15
steering_coeff = 1.5

def surgical_forward(input_ids, cache, last_id_only=True):
    """
    –†—É—á–Ω–æ–π forward pass –¥–ª—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∏–Ω—ä–µ–∫—Ü–∏–∏.
    """
    # 1. –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    hidden_state = model.modules[0].forward(input_ids)

    # 2. –ò—Ç–µ—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ —Å–ª–æ–∏ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞
    # model.modules —Å–æ–¥–µ—Ä–∂–∏—Ç [Embeddings, Layer0, Layer1, ..., Norm, Head]
    for i, module in enumerate(model.modules[1:-2]): # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º emb, norm, head
        layer_idx = i
        
        # --- –¢–û–ß–ö–ê –í–ú–ï–®–ê–¢–ï–õ–¨–°–¢–í–ê (INJECTION POINT) ---
        if layer_idx == target_layer_idx:
            # –î–æ–±–∞–≤–ª—è–µ–º –≤–µ–∫—Ç–æ—Ä –∫ —Å–∫—Ä—ã—Ç–æ–º—É —Å–æ—Å—Ç–æ—è–Ω–∏—é
            hidden_state += steering_vector * steering_coeff
        # ---------------------------------------------
        
        # –ó–∞–ø—É—Å–∫ —Å–ª–æ—è
        hidden_state = module.forward(hidden_state, cache, attention_mask=None)

    # 3. –§–∏–Ω–∞–ª—å–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏ –ì–æ–ª–æ–≤–∞ (Head)
    hidden_state = model.modules[-2].forward(hidden_state) # RMSNorm
    logits = model.modules[-1].forward(hidden_state)       # Head
    
    return logits
```

---

## 5. üîå API –°–õ–û–ô (FastAPI)

```python
from fastapi import FastAPI
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
import json

app = FastAPI()
# orchestrator = CognitiveOrchestrator(...)

class ChatRequest(BaseModel):
    messages: list
    model: str = "max-orchestrator"

@app.post("/v1/chat/completions")
async def chat_completions(req: ChatRequest):
    # –õ–æ–≥–∏–∫–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ —Å–æ–æ–±—â–µ–Ω–∏–π –≤ –ø—Ä–æ–º–ø—Ç...
    prompt = "converted_prompt"
    
    async def stream_generator():
        # async for token in orchestrator.generate_orchestrated(prompt):
        #     yield f"data: {json.dumps({'choices': [{'delta': {'content': token}}]})}\n\n"
        yield "data: [DONE]\n\n"
        
    return StreamingResponse(stream_generator(), media_type="text/event-stream")
```

---

## 6. ‚öîÔ∏è –ö–û–ù–ö–£–†–ï–ù–¢–ù–´–ô –ê–ù–ê–õ–ò–ó: MAX vs LM STUDIO (–ò–°–°–õ–ï–î–û–í–ê–ù–ò–ï 2026)

–ù–∞—à–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ LM Studio (v0.3.x / Roadmap 2026) –≤—ã—è–≤–∏–ª–æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ **MAX Engine** –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø—Ä–µ–æ–¥–æ–ª–µ—Ç—å.

| –§—É–Ω–∫—Ü–∏—è | LM Studio (2026) | üèóÔ∏è MAX Engine (–ù–∞—à) |
| :--- | :--- | :--- |
| **–î–≤–∏–∂–æ–∫ (Core)** | `llama.cpp` (GGUF) / MLX | **`ExLlamaV2` (EXL2)** |
| **–°–∫–æ—Ä–æ—Å—Ç—å (4070 Ti)** | –•–æ—Ä–æ—à–∞—è, –Ω–æ —Ä–µ–¥–∫–æ –Ω–∞–≥—Ä—É–∂–∞–µ—Ç CUDA –Ω–∞ 100% | **SOTA** (–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ C++ —è–¥—Ä–∞) |
| **–ú—É–ª—å—Ç–∏-–º–æ–¥–µ–ª—å** | –¢–æ–ª—å–∫–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞. API –≤—ã–∑–æ–≤—ã –æ—Ç–¥–µ–ª—å–Ω—ã–µ. | **–í–Ω—É—Ç—Ä–∏–ø—Ä–æ—Ü–µ—Å—Å–Ω–∞—è –û—Ä–∫–µ—Å—Ç—Ä–∞—Ü–∏—è**. Zero-copy –æ–±–º–µ–Ω —Ç–µ–Ω–∑–æ—Ä–∞–º–∏. |
| **–í–º–µ—à–∞—Ç–µ–ª—å—Å—Ç–≤–æ** | **–ù–µ—Ç**. API ‚Äî "—á–µ—Ä–Ω—ã–π —è—â–∏–∫". | **–ì–ª—É–±–æ–∫–∞—è –•–∏—Ä—É—Ä–≥–∏—è**. –•—É–∫–∏ –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Å–ª–æ–∏ (–Ω–∞–ø—Ä., Layer 15). |
| **–ó–∞–¥–µ—Ä–∂–∫–∞ (Latency)** | +20ms –Ω–∞ –∫–∞–∂–¥—ã–π HTTP –≤—ã–∑–æ–≤ (Localhost) | **In-Memory** (<1ms —Ü–∏–∫–ª –ø—Ä–æ–≤–µ—Ä–∫–∏) |
| **Steering (–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ)** | –ù–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è. | **–ù–∞—Ç–∏–≤–Ω–æ–µ**. `hidden_states += vector`. |
| **–ö–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ** | GGUF (–°–º–µ—à–∞–Ω–Ω–æ–µ CPU/GPU) | **EXL2 (4-bit)**. –°–æ–∑–¥–∞–Ω–æ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –¥–ª—è —á–∏—Å—Ç–æ–≥–æ GPU. |

### üöÄ –ü–û–ß–ï–ú–£ MAX –ü–û–ë–ï–ñ–î–ê–ï–¢

1. **–ó–∞–¥–µ—Ä–∂–∫–∞ "–î–∏—Ä–∏–∂–µ—Ä–∞"**: –í LM Studio, –µ—Å–ª–∏ –º—ã –∑–∞—Ö–æ—Ç–∏–º —Å–¥–µ–ª–∞—Ç—å Python-—Å–∫—Ä–∏–ø—Ç "–æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä", –º—ã –±—É–¥–µ–º –ø–ª–∞—Ç–∏—Ç—å —Ü–µ–Ω—É HTTP-–∑–∞–ø—Ä–æ—Å–∞ (–°–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏—è -> –û—Ç–ø—Ä–∞–≤–∫–∞ -> –ò–Ω—Ñ–µ—Ä–µ–Ω—Å -> –ü—Ä–∏–µ–º -> –î–µ—Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏—è) *–Ω–∞ –∫–∞–∂–¥–æ–º —Ç–æ–∫–µ–Ω–µ*. MAX Engine –∑–∞–ø—É—Å–∫–∞–µ—Ç –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ü–∏—é –≤ **—Ç–æ–º –∂–µ Python-–ø—Ä–æ—Ü–µ—Å—Å–µ**, –≥–¥–µ –∫—Ä—É—Ç—è—Ç—Å—è CUDA —è–¥—Ä–∞. –ú–∞–ª–∞—è –º–æ–¥–µ–ª—å —á–∏—Ç–∞–µ—Ç —Ç–µ–Ω–∑–æ—Ä—ã –ë–æ–ª—å—à–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞–ø—Ä—è–º—É—é –∏–∑ VRAM –±–µ–∑ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è –≤ –æ–ø–µ—Ä–∞—Ç–∏–≤–Ω—É—é –ø–∞–º—è—Ç—å.
2. **–ù–∞—Å—Ç–æ—è—â–∞—è –ù–µ–π—Ä–æ—Ö–∏—Ä—É—Ä–≥–∏—è**: LM Studio ‚Äî —ç—Ç–æ —á–µ—Ä–Ω—ã–π —è—â–∏–∫. –í—ã –Ω–µ –º–æ–∂–µ—Ç–µ —Å–∫–∞–∑–∞—Ç—å "–î–æ–±–∞–≤—å —ç—Ç–æ—Ç –≤–µ–∫—Ç–æ—Ä –∫ —Å–ª–æ—é 15" —á–µ—Ä–µ–∑ –∏—Ö API. MAX Engine –¥–∞–µ—Ç –Ω–∞–º –ø—Ä—è–º–æ–π –¥–æ—Å—Ç—É–ø –∫ –æ–±—ä–µ–∫—Ç—É `model.modules` –∏–∑ Python.
3. **–£—Ç–∏–ª–∏–∑–∞—Ü–∏—è VRAM**: LM Studio ‚Äî —ç—Ç–æ –∫–æ–º–±–∞–π–Ω "–¥–ª—è –≤—Å–µ—Ö", –æ–Ω–∞ —á–∞—Å—Ç–æ –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–∞–º—è—Ç—å. MAX Engine **–∂–µ—Å—Ç–∫–æ –∑–∞—Ç–æ—á–µ–Ω** –ø–æ–¥ –Ω–∞—à –±—é–¥–∂–µ—Ç –≤ 16GB VRAM, –ø–æ–∑–≤–æ–ª—è—è –≤–ø–∏—Ö–Ω—É—Ç—å `Qwen-7B` + `Qvikhr-1.5B` + `ContextMirror` —Å —Ç–æ—á–Ω–æ—Å—Ç—å—é –¥–æ –º–µ–≥–∞–±–∞–π—Ç–∞ –±–ª–∞–≥–æ–¥–∞—Ä—è EXL2 –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—é.
