Архитектурные стратегии реализации параллельных микроагентов на потребительском оборудовании: Максимизация эффективности 16 ГБ VRAM
Введение: Смена парадигмы от последовательного к параллельному инференсу
В современной экосистеме искусственного интеллекта наблюдается фундаментальный сдвиг от использования единичных монолитных чат-ботов к оркестрации сложных мультиагентных систем. Разработчики и исследователи всё чаще сталкиваются с необходимостью одновременного выполнения нескольких независимых когнитивных задач — например, когда один агент генерирует программный код, второй параллельно пишет документацию, а третий осуществляет критический анализ полученных результатов. Пользовательский запрос ставит важнейший архитектурный вопрос: возможно ли на одной видеокарте с объемом памяти 16 ГБ, такой как NVIDIA GeForce RTX 4080 или мобильная версия RTX 4090, реализовать "расщепление" одной модели на два-три независимых микроагента, работающих не последовательно, а параллельно?
Ответ на этот вопрос утвердительный, однако он требует глубокого понимания механики работы современных движков инференса и математики управления памятью GPU. Традиционное представление о том, что для двух агентов требуются две копии модели, ошибочно и технически нереализуемо на потребительском железе. Решение кроется не в физическом дублировании весов нейросети, а в логической виртуализации контекста через механизмы Continuous Batching (непрерывной пакетной обработки) и KV Cache Paging (страничной организации кэша ключей и значений). При таком подходе вычислительное ядро (веса модели) остается единым и неизменным, в то время как состояние каждого агента (его "память" и текущий диалог) хранится в изолированных слотах памяти.
В данном отчете представлен исчерпывающий анализ архитектурных решений, позволяющих реализовать такую систему. Мы подробно рассмотрим математические ограничители 16 ГБ VRAM, сравним эффективность различных движков (Ollama, llama.cpp, vLLM) в условиях ограниченных ресурсов, разберем программную реализацию асинхронности через LangGraph и проанализируем влияние квантования и архитектурных особенностей моделей (таких как Grouped Query Attention) на возможность параллелизма.
________________
1. Физика ограничений 16 ГБ VRAM: Математический аудит
Для того чтобы понять, как "вместить" несколько агентов в 16 ГБ видеопамяти, необходимо провести детальную декомпозицию потребления ресурсов. Видеопамять (VRAM) при работе с большими языковыми моделями (LLM) расходуется на три основных компонента, каждый из которых имеет свою динамику роста. Понимание этих пропорций является ключом к успешному развертыванию мультиагентных систем.
1.1 Статический вес модели и императив квантования
Первым и самым крупным потребителем памяти являются сами веса модели — параметры нейронной сети, которые загружаются в VRAM и остаются неизменными в процессе работы (read-only). Для модели размером 14 миллиардов параметров (например, Qwen-2.5-14B, которая является текущим стандартом эффективности для карт среднего сегмента), потребление памяти в стандартном формате FP16 (16-битная точность с плавающей запятой) составляет колоссальные величины. Поскольку каждый параметр в FP16 занимает 2 байта, чистый вес модели составляет $14.7 \times 10^9 \times 2 \approx 29.4$ ГБ. Это значение почти вдвое превышает физический лимит 16 ГБ карты, делая запуск модели в оригинальном разрешении невозможным даже для одного агента, не говоря уже о параллельных процессах.1
Следовательно, использование квантования (сжатия весов) становится не опцией, а жесткой необходимостью. Современные методы квантования, такие как GGUF (используемый в llama.cpp/Ollama) или EXL2, позволяют снизить точность весов до 4-8 бит с минимальной потерей качества генерации. Рассмотрим математику для популярного формата Q4_K_M (смешанное 4-битное квантование), который обеспечивает оптимальный баланс скорости и качества:


$$\text{Размер}_{Q4} \approx P \times 0.56 \text{ байт} + \text{Метаданные}$$
Для модели 14B это дает приблизительно 8.2–9.0 ГБ занятой памяти. К этому значению необходимо добавить накладные расходы среды выполнения CUDA (context overhead), которые обычно составляют от 0.5 до 1.0 ГБ в зависимости от операционной системы и версии драйверов. Таким образом, только "холодный старт" системы с загруженной моделью Qwen-2.5-14B "съедает" около 10 ГБ из доступных 16 ГБ. Это оставляет критически важный, но ограниченный бюджет в 6 ГБ для динамических операций — кэша контекста (KV Cache) и буферов активации, которые и определяют возможность параллельной работы агентов.3
1.2 Динамика KV-кэша: Ключ к параллелизму
Именно здесь происходит "магия" разделения модели на микроагентов. KV-кэш (Key-Value Cache) — это структура данных, хранящая промежуточные результаты вычислений механизма внимания (Attention) для всех предыдущих токенов. Без кэша модели пришлось бы пересчитывать весь контекст заново для генерации каждого нового слова, что сделало бы работу катастрофически медленной.
В архитектуре с одним весовым ядром и несколькими агентами, каждый агент получает свой собственный, изолированный слот KV-кэша. Если мы хотим запустить три агента одновременно, нам нужно выделить память под три независимых потока истории. Формула расчета памяти на один токен контекста выглядит следующим образом:


$$M_{token} = 2 \times n_{layers} \times n_{kv\_heads} \times d_{head} \times P_{cache}$$
Где:
* $2$ — коэффициент для ключей (Keys) и значений (Values).
* $n_{layers}$ — количество слоев в модели (для Qwen-2.5-14B это 48).
* $n_{kv\_heads}$ — количество голов внимания для KV. Здесь критическую роль играет архитектура GQA (Grouped Query Attention). В старых моделях (например, Llama-1) количество KV-голов совпадало с количеством голов запросов ($n_{heads} = 40$), что приводило к огромному расходу памяти. В современных моделях, таких как Qwen-2.5 или Mistral, используется GQA, где $n_{kv\_heads}$ значительно меньше (например, 8). Это снижает потребление памяти кэшем в 5-8 раз.1
* $d_{head}$ — размерность одной головы (обычно 128).
* $P_{cache}$ — точность кэша (обычно FP16, т.е. 2 байта, даже если веса модели в int4).
Подставим значения для Qwen-2.5-14B:




$$M_{token} = 2 \times 48 \times 8 \times 128 \times 2 = 196,608 \text{ байт} \approx 0.19 \text{ МБ/токен}$$
Имея свободный бюджет в 6000 МБ (после загрузки весов), мы можем рассчитать общую емкость контекста:




$$\text{Total Context Capacity} \approx \frac{6000}{0.19} \approx 31,500 \text{ токенов}$$
Это число — 31,500 токенов — является нашим "золотым запасом". Его можно распределить между агентами любым способом. Например:
* Сценарий А (2 агента): Каждый получает по 15,000 токенов контекста. Этого достаточно для обработки больших документов или длинных диалогов.
* Сценарий Б (4 агента): Каждый получает по ~7,500 токенов. Этого достаточно для стандартных задач кодинга или чата.
Если бы модель не использовала GQA (как старые версии Llama 2 13B), потребление памяти было бы ~1 МБ на токен, и общий бюджет составил бы всего ~6000 токенов, что сделало бы параллельную работу нескольких агентов практически невозможной на 16 ГБ.6
1.3 Буферы активации и скрытые расходы
Третий компонент — буферы активации (Activation Buffers) — часто упускается из виду, но он критичен именно для пакетной обработки (batch processing). Когда GPU обрабатывает запросы от двух агентов одновременно, он создает временные тензоры для хранения результатов промежуточных слоев. Размер этих буферов зависит от размера пакета (batch size) и максимальной длины последовательности.
Формула оценки памяти активаций сложна, но эмпирическое правило для инференса гласит, что необходимо резервировать примерно 0.5–1.0 ГБ памяти под "scratchpad" (черновик) для вычислений, особенно при использовании длинных контекстов и больших размеров пакета. При использовании агрессивных настроек параллелизма (NUM_PARALLEL > 4) потребление памяти активациями может резко возрасти, вызывая ошибки OOM (Out Of Memory) даже при наличии свободного места под кэш. Поэтому для 16 ГБ карты рекомендация ограничивать количество параллельных слотов числом 2-4 является не просто программной настройкой, а аппаратной необходимостью, продиктованной балансом между памятью для хранения истории (кэш) и памятью для вычислений (активации).4
Компонент памяти
	Оценка для Qwen-2.5-14B (16GB VRAM)
	Доступность для параллелизма
	Веса модели (Q4_K_M)
	~9.0 ГБ
	Фиксировано (загружается 1 раз)
	Системный оверхед (CUDA)
	~0.7 ГБ
	Фиксировано
	Буферы активации
	~1.0 ГБ
	Динамически (зависит от batch size)
	Доступно для KV-кэша
	~5.3 ГБ
	Ресурс для разделения между агентами
	Макс. токенов (GQA)
	~27,000
	Можно разделить на 3 агента по 9k
	Макс. токенов (Non-GQA)
	~5,000
	Недостаточно для мультиагентности
	Этот анализ подтверждает, что с математической точки зрения реализация 2-3 независимых микроагентов на 16 ГБ VRAM абсолютно реалистична, при условии использования современной архитектуры модели с GQA и квантования весов.
________________
2. Архитектура движков инференса: Механизм "расщепления"
Поняв аппаратные возможности, перейдем к программной реализации. Как именно "одна операция за раз" превращается в "две-три за раз"? Это достигается за счет архитектурного паттерна, известного как Continuous Batching (непрерывная пакетная обработка) или Cellular Batching.
2.1 Концепция слотов (Slots) и очередей
В стандартном режиме (например, при локальном запуске через простой скрипт Python) модель работает в режиме "один вход — один выход". Если поступает второй запрос, он блокируется до завершения первого. Однако современные серверы инференса, такие как Ollama (построенный на базе llama.cpp) и vLLM, работают иначе.
Они реализуют концепцию "слотов" (slots) — виртуальных контейнеров для контекста. Когда мы запускаем сервер с параметром параллелизма (например, 4), движок аллоцирует в памяти 4 независимых области под KV-кэш.
* Агент А отправляет запрос -> Сервер назначает ему Слот ID 1.
* Агент Б отправляет запрос -> Сервер назначает ему Слот ID 2.
Процессор GPU обладает огромным количеством ядер (например, 9728 CUDA-ядер у RTX 4080). Вычисление матричного умножения для одного токена одного агента часто не загружает GPU на 100%. Пакетная обработка позволяет объединить вычисления для Агента А и Агента Б в одну матричную операцию.
На такте $T_1$ модель предсказывает следующее слово для Агента А и следующее слово для Агента Б практически одновременно. Для внешнего наблюдателя (и для кода на Python) это выглядит как истинная параллельность. Токены поступают в оба потока асинхронно.9
2.2 Ollama и llama.cpp: Короли потребительского сегмента
Для пользователя с 16 ГБ VRAM выбор движка имеет критическое значение. Несмотря на то, что vLLM является стандартом в индустрии для высоконагруженных серверов благодаря технологии PagedAttention, он обладает существенным недостатком для 16 ГБ карт: высоким потреблением памяти "на старте". vLLM по умолчанию резервирует 90% VRAM под кэш и имеет значительный оверхед на структуры управления памятью, что часто приводит к невозможности загрузить 14B модель вместе с достаточным контекстом. Кроме того, vLLM жестко привязан к формату тензоров и не поддерживает эластичное "вытеснение" слоев в оперативную память (RAM offloading) в случае переполнения.11
Ollama (и лежащий в её основе llama.cpp), напротив, оптимизирована именно для таких сценариев.
1. GGUF формат: Позволяет эффективно упаковывать веса и кэш.
2. Гибкое управление памятью: Если контекст трех агентов внезапно превысит 16 ГБ, Ollama не "упадет" с ошибкой OOM, как vLLM. Она прозрачно начнет выгружать часть слоев или кэша в системную RAM (CPU). Это замедлит генерацию (токены пойдут медленнее), но не прервет работу агентов, что критически важно для надежности локальных систем.13
3. Настройка OLLAMA_NUM_PARALLEL: Эта переменная среды напрямую управляет количеством слотов. Установка OLLAMA_NUM_PARALLEL=4 сообщает серверу: "Будь готов обрабатывать 4 независимых диалога одновременно". При этом OLLAMA_MAX_LOADED_MODELS=1 гарантирует, что мы не попытаемся загрузить вторую модель и не вытесним текущую.15
2.3 Сравнительный анализ производительности
При переходе от последовательной обработки к параллельной меняется профиль производительности системы. Важно понимать разницу между Latency (задержкой) и Throughput (пропускной способностью).
* Последовательный режим: Агент 1 генерирует ответ за 10 секунд. Агент 2 ждет 10 секунд, затем генерирует свой ответ за 10 секунд. Общее время: 20 секунд.
* Параллельный режим (2 слота): Оба агента начинают генерацию одновременно. Из-за конкуренции за ресурсы памяти и вычислительные ядра скорость генерации каждого замедляется (например, до 14 секунд). Однако оба ответа готовы через 14 секунд.
   * Выигрыш: Общее время выполнения задачи сократилось на 30% (с 20 до 14 сек). Пропускная способность (tokens per second, TPS) всей системы выросла.
   * Плата: Индивидуальная скорость генерации ("ощущение" скорости в чате) упала. Для фоновых агентов (микросервисов) это не имеет значения, но для интерактивного чата это может быть заметно.12
В бенчмарках на RTX 3090/4090 наблюдается эффект "йо-йо" в загрузке GPU при параллельном инференсе. Загрузка колеблется между 40% и 80%, так как планировщик (scheduler) динамически формирует пакеты токенов. Это нормальное поведение, свидетельствующее о работе механизма continuous batching.10
________________
3. Оркестрация микроагентов: Программная реализация
Наличие слотов в Ollama — это лишь фундамент. Чтобы "оживить" микроагентов, необходим управляющий слой, который будет отправлять запросы в эти слоты. Здесь на сцену выходят фреймворки оркестрации, такие как LangGraph.
3.1 Проблема синхронного блокирования (GIL)
Типичная ошибка новичков — использование стандартных синхронных вызовов Python (response = model.invoke(...)) в цикле или потоках. Python, из-за глобальной блокировки интерпретатора (GIL), часто не может эффективно распараллелить сетевые запросы в таком режиме, превращая параллелизм в последовательность.
Для реализации истинной параллельности необходимо использовать асинхронное программирование (asyncio) и соответствующие методы библиотек (ainvoke, abatch).
3.2 Архитектурный паттерн "Асинхронный Веер" (Async Fan-Out)
Рассмотрим классическую задачу "Map-Reduce", где один "Супервизор" разбивает задачу на подзадачи для микроагентов.
Сценарий: Пользователь просит: "Проанализируй этот отчет, напиши по нему код на Python и составь документацию".
1. Супервизор: Определяет две задачи: Task_Coding и Task_Docs.
2. Ветвление (Fan-Out): Система запускает два асинхронных узла графа.
   * Узел "Кодер" отправляет запрос в Ollama.
   * Узел "Писатель" отправляет запрос в Ollama.
3. Обработка в Ollama: Сервер видит два входящих HTTP-запроса. Поскольку OLLAMA_NUM_PARALLEL=4, он не ставит второй запрос в очередь, а начинает обрабатывать их одновременно в слотах 1 и 2.
4. Агрегация (Reduce): Узлы ожидают завершения (await), собирают результаты и передают их финальному агенту для сборки ответа.
Пример реализации на LangGraph (концептуальный код):


Python




import asyncio
from langchain_ollama import ChatOllama
from langgraph.graph import StateGraph, END
from langchain_core.messages import SystemMessage, HumanMessage

# Инициализация модели. 
# Важно: num_ctx должен быть рассчитан так, чтобы (num_ctx * OLLAMA_NUM_PARALLEL) < VRAM.
# Для 16GB и 4 слотов безопасным будет 4096 или 8192 (с квантованным кэшем).
llm = ChatOllama(
   model="qwen2.5:14b",
   temperature=0.2,
   num_ctx=8192,  
   num_predict=512 
)

# Определение состояния графа
class State(TypedDict):
   topic: str
   code: str
   docs: str

# Асинхронные функции узлов
async def coder_node(state):
   # Этот вызов не блокирует выполнение программы
   msg =}")]
   response = await llm.ainvoke(msg)
   return {"code": response.content}

async def writer_node(state):
   # Запускается параллельно с coder_node
   msg =}")]
   response = await llm.ainvoke(msg)
   return {"docs": response.content}

# Построение графа
builder = StateGraph(State)
builder.add_node("coder", coder_node)
builder.add_node("writer", writer_node)

# Параллельный запуск
builder.set_entry_point("coder") # В реальности здесь нужен маршрутизатор или параллельный старт
# LangGraph позволяет запускать узлы параллельно, если они не зависят друг от друга
# и находятся на одном уровне выполнения (Super-step).

Критически важно: методы ainvoke и abatch используют asyncio.gather под капотом (в реализации LangChain), что позволяет отправить оба запроса в Ollama практически мгновенно. Если использовать invoke, второй агент начнет работу только после того, как первый закончит генерацию всех токенов.19
3.3 Управление состоянием и "Гонка данных"
При параллельной работе микроагентов возникает проблема согласованности данных. LangGraph решает её через механизм "супер-шагов" (super-steps). Узлы, запущенные параллельно, читают одно и то же начальное состояние, выполняют работу и возвращают свои обновления ("патчи"). Система синхронизирует эти обновления перед переходом к следующему шагу. Это предотвращает ситуации, когда один агент перезаписывает данные другого. Для 16 ГБ VRAM это означает, что состояние графа (текстовые данные) хранится в оперативной памяти (RAM), и только активный контекст инференса находится в VRAM.22
________________
4. Продвинутые техники: Адаптеры (LoRA) и Системные промпты
Пользователь спрашивает о "разбиении" на независимых агентов. Существует два уровня такого разбиения: "Мягкое" (через промпты) и "Жесткое" (через веса).
4.1 Системные промпты: Эффективный и дешевый метод
Самый простой способ создать иллюзию разных агентов — использовать разные системные инструкции для одного и того же базового чекпоинта модели.
* Слот 1 получает контекст: System: "Ты эксперт по SQL..."
* Слот 2 получает контекст: System: "Ты психолог..."
Для Ollama это просто разные последовательности токенов в разных слотах. Это идеально работает на 16 ГБ, так как веса модели (9 ГБ) общие, и тратится только память на KV-кэш для текста инструкций. Более того, Ollama поддерживает Prefix Caching (кэширование префикса). Если у всех агентов есть общая вводная часть (например, "Ты полезный ассистент компании X..."), она обрабатывается и кэшируется один раз, экономя память и время префилла (prefill).24
4.2 Multi-LoRA: Истинная специализация
Более сложный метод — использование LoRA (Low-Rank Adaptation). Адаптеры — это небольшие файлы весов (100-200 МБ), которые "дообучают" модель под конкретную задачу.
Можно ли запустить Base_Model + LoRA_SQL и Base_Model + LoRA_Python одновременно?
* vLLM: Поддерживает это нативно и очень эффективно. Адаптеры хранятся в CPU и подгружаются в слои внимания "на лету".26
* Ollama/llama.cpp: Ситуация сложнее. По состоянию на 2024-2025 годы, поддержка динамического переключения LoRA в рантайме (per-request) в Ollama находится в стадии активной разработки и часто требует перезагрузки модели для смены адаптера, что убивает идею параллелизма. В последних версиях llama.cpp появилась поддержка параметра --lora для каждого запроса, но интеграция этого в API Ollama не всегда стабильна.
   * Рекомендация для 16 ГБ: На текущий момент надежнее использовать мощную универсальную модель (Qwen-2.5-14B-Instruct) и специализировать её промптами, чем пытаться жонглировать адаптерами в памяти, рискуя стабильностью и перерасходом VRAM.28 Если специализация критична, лучше использовать один "мердж" (слитую модель), содержащий навыки обеих областей, или выделить отдельные серверные процессы на разных портах (что невозможно на одной карте из-за дублирования весов).
________________
5. Практическое руководство по настройке (Environment Setup)
Для реализации описанной архитектуры на машине с 16 ГБ VRAM необходимо выполнить следующую конфигурацию.
5.1 Переменные окружения
Перед запуском сервера Ollama необходимо задать следующие параметры (в Linux/macOS через export, в Windows через системные настройки):


Bash




# Разрешить 4 параллельных потока обработки
# Это создаст 4 слота KV-кэша.
export OLLAMA_NUM_PARALLEL=4

# Ограничить очередь, чтобы не перегружать сервер запросами, если слоты заняты
export OLLAMA_MAX_QUEUE=512

# Запретить загрузку более одной модели одновременно.
# Это критично, чтобы случайно не выгрузить Qwen из памяти при запросе другой модели.
export OLLAMA_MAX_LOADED_MODELS=1

# Опционально: Настройка Flash Attention (обычно включено по умолчанию для поддерживаемых моделей)
export OLLAMA_FLASH_ATTENTION=1

5.2 Выбор модели и управление контекстом
Ключевой момент — контроль длины контекста. По умолчанию Ollama может пытаться выделить 2048 или 4096 токенов. Если вы зададите num_ctx слишком большим, вы получите OOM.
Рекомендуемый Modelfile для микроагентов:


Dockerfile




FROM qwen2.5:14b
# Устанавливаем окно контекста в 8192.
# При 4 параллельных слотах это потребует:
# 4 * 8192 * ~0.2 МБ ≈ 6.5 ГБ VRAM.
# 9 ГБ (Веса) + 6.5 ГБ (Кэш) = 15.5 ГБ. Идеально под 16 ГБ лимит.
PARAMETER num_ctx 8192
PARAMETER temperature 0.1
SYSTEM "Ты автономный микроагент, работающий в составе кластера."

Если возникают ошибки памяти, можно снизить num_ctx до 4096 или уменьшить OLLAMA_NUM_PARALLEL до 2 или 3.
5.3 Квантование KV-кэша (Продвинутая техника)
В последних версиях llama.cpp (и Ollama) появилась возможность квантовать не только веса, но и сам KV-кэш (например, в формат FP8 или Q8_0). Это позволяет сократить потребление памяти кэшем почти в 2 раза с минимальной потерей качества "вспоминания" фактов.
Для активации этой функции (если она доступна в вашей версии Ollama) используется параметр типа кэша ct_k и ct_v (cache type key/value). Это может позволить увеличить контекст каждого агента до 16к или 32к токенов на той же 16 ГБ карте.30
________________
6. Заключение
Реализация двух-трех независимых микроагентов на одной видеокарте с 16 ГБ памяти — это не только теоретически возможная, но и практически эффективная задача. Ключ к успеху лежит в отказе от ментальной модели "один процесс = одна модель" и переходе к модели "один движок = множество слотов контекста".
Используя модель класса 14B (например, Qwen-2.5) с квантованием Q4_K_M и архитектурой GQA, вы занимаете около 9-10 ГБ статической памяти, оставляя ~6 ГБ для динамического маневра. Механизмы OLLAMA_NUM_PARALLEL позволяют нарезать эти 6 ГБ на несколько независимых "дорожек", по которым движутся мыслительные процессы ваших агентов. Оркестратор типа LangGraph асинхронно подает данные на эти дорожки, а GPU обрабатывает их пакетами, обеспечивая высокую общую пропускную способность системы.
Таким образом, 16 ГБ VRAM превращаются из ограничивающего фактора в достаточную платформу для запуска полноценного мультиагентного "роя", способного решать сложные составные задачи без необходимости инвестировать в промышленное оборудование уровня A100/H100.
Сводная таблица рекомендаций
Параметр
	Значение
	Обоснование
	Модель
	Qwen-2.5-14B-Instruct-Q4_K_M
	Оптимальный баланс интеллекта и размера. GQA экономит память кэша.
	Движок
	Ollama (backend llama.cpp)
	Эффективное использование памяти, возможность оффлоада в RAM.
	Параллелизм
	OLLAMA_NUM_PARALLEL=3 или 4
	Позволяет запускать 3-4 агента. Оставляет резерв памяти.
	Контекст
	4096 - 8192 токенов
	Безопасный предел для предотвращения OOM на 16 ГБ.
	Оркестрация
	Python asyncio + LangGraph
	Необходима для неблокирующей отправки запросов.
	Специализация
	Системные промпты
	Более стабильно и экономно по памяти, чем LoRA.
	Источники
1. unsloth/Qwen2.5-14B-Instruct - Hugging Face, дата последнего обращения: декабря 13, 2025, https://huggingface.co/unsloth/Qwen2.5-14B-Instruct
2. Calculating Memory Footprint for Large Language Models (LLMs): A Complete Guide, дата последнего обращения: декабря 13, 2025, https://medium.com/@hexiangnan/calculating-memory-footprint-for-large-language-models-llms-a-complete-guide-98ac3fdfdbf6
3. Calculating GPU memory for serving LLMs | LLM Inference Handbook - BentoML, дата последнего обращения: декабря 13, 2025, https://bentoml.com/llm/getting-started/calculating-gpu-memory-for-llms
4. manuelescobar-dev/LLM-Tools: Open-source calculator for LLM system requirements. - GitHub, дата последнего обращения: декабря 13, 2025, https://github.com/manuelescobar-dev/LLM-Tools
5. nvidia/Mistral-NeMo-12B-Instruct - Hugging Face, дата последнего обращения: декабря 13, 2025, https://huggingface.co/nvidia/Mistral-NeMo-12B-Instruct
6. erans/selfhostllm: A web-based calculator for estimating GPU memory requirements and maximum concurrent requests for self-hosted LLM inference. - GitHub, дата последнего обращения: декабря 13, 2025, https://github.com/erans/selfhostllm
7. LLM Inference Sizing and Performance Guidance - VMware Cloud Foundation (VCF) Blog, дата последнего обращения: декабря 13, 2025, https://blogs.vmware.com/cloud-foundation/2024/09/25/llm-inference-sizing-and-performance-guidance/
8. Estimating LLM Inference Memory Requirements | by Kyle Bell - Medium, дата последнего обращения: декабря 13, 2025, https://medium.com/@kylebell_70950/estimating-llm-inference-memory-requirements-fa9523fb4808
9. Ollama 0.2 — revolutionizing local model management with concurrency | by Simeon Emanuilov | Medium, дата последнего обращения: декабря 13, 2025, https://medium.com/@simeon.emanuilov/ollama-0-2-revolutionizing-local-model-management-with-concurrency-2318115ce961
10. OLLAMA_NUM_PARALLEL and resource usage : r/ollama - Reddit, дата последнего обращения: декабря 13, 2025, https://www.reddit.com/r/ollama/comments/1do1602/ollama_num_parallel_and_resource_usage/
11. vLLM vs Llama.cpp vs Ollama: Multi-GPU LLM Performance - Arsturn, дата последнего обращения: декабря 13, 2025, https://www.arsturn.com/blog/multi-gpu-showdown-benchmarking-vllm-llama-cpp-ollama-for-maximum-performance
12. vLLM or llama.cpp: Choosing the right LLM inference engine for your use case, дата последнего обращения: декабря 13, 2025, https://developers.redhat.com/articles/2025/09/30/vllm-or-llamacpp-choosing-right-llm-inference-engine-your-use-case
13. llama.cpp guide - Running LLMs locally, on any hardware, from scratch ::, дата последнего обращения: декабря 13, 2025, https://steelph0enix.github.io/posts/llama-cpp-guide/
14. Stop Wasting Your Multi-GPU Setup With llama.cpp: Use vLLM or ExLlamaV2 for Tensor Parallelism : r/LocalLLaMA - Reddit, дата последнего обращения: декабря 13, 2025, https://www.reddit.com/r/LocalLLaMA/comments/1ijw4l5/stop_wasting_your_multigpu_setup_with_llamacpp/
15. OLLAMA Memory Usage Spike During Manual AI Analysis · Issue #157 · clusterzx/paperless-ai - GitHub, дата последнего обращения: декабря 13, 2025, https://github.com/clusterzx/paperless-ai/issues/157
16. FAQ - Ollama's documentation, дата последнего обращения: декабря 13, 2025, https://docs.ollama.com/faq
17. Decoding Real-Time LLM Inference: A Guide to the Latency vs. Throughput Bottleneck | by Nadeem Khan(NK) | LearnWithNK | Oct, 2025 | Medium, дата последнего обращения: декабря 13, 2025, https://medium.com/learnwithnk/decoding-real-time-llm-inference-a-guide-to-the-latency-vs-throughput-bottleneck-c1ad96442d50
18. High RAM usage causes yo-yoing memory pressure on Mac, slow inference #4151 - GitHub, дата последнего обращения: декабря 13, 2025, https://github.com/ollama/ollama/issues/4151
19. ChatOllama — LangChain documentation, дата последнего обращения: декабря 13, 2025, https://python.langchain.com/api_reference/ollama/chat_models/langchain_ollama.chat_models.ChatOllama.html?__hstc=5909356.73bd3bee6fa385653ecd7c9674ba06f0.1755043200183.1755043200184.1755043200185.1&__hssc=5909356.1.1755043200186&__hsfp=2324370431
20. LangChain in Chains #53: Advanced Ollama | by Okan Yenigün | Towards Dev - Medium, дата последнего обращения: декабря 13, 2025, https://medium.com/towardsdev/langchain-in-chains-53-advanced-ollama-abd90114561a
21. Leveraging LangGraph's Send API for Dynamic and Parallel Workflow Execution, дата последнего обращения: декабря 13, 2025, https://dev.to/sreeni5018/leveraging-langgraphs-send-api-for-dynamic-and-parallel-workflow-execution-4pgd
22. Best practices for parallel nodes (fanouts) - LangGraph - LangChain Forum, дата последнего обращения: декабря 13, 2025, https://forum.langchain.com/t/best-practices-for-parallel-nodes-fanouts/1900
23. Graph API overview - Docs by LangChain, дата последнего обращения: декабря 13, 2025, https://docs.langchain.com/oss/python/langgraph/graph-api
24. How to use multiple system-prompts - ollama - Reddit, дата последнего обращения: декабря 13, 2025, https://www.reddit.com/r/ollama/comments/1kaw1sl/how_to_use_multiple_systemprompts/
25. Getting ollama to work with very long system prompts - Reddit, дата последнего обращения: декабря 13, 2025, https://www.reddit.com/r/ollama/comments/1egddvv/getting_ollama_to_work_with_very_long_system/
26. Efficient and cost-effective multi-tenant LoRA serving with Amazon SageMaker - AWS, дата последнего обращения: декабря 13, 2025, https://aws.amazon.com/blogs/machine-learning/efficient-and-cost-effective-multi-tenant-lora-serving-with-amazon-sagemaker/
27. Improving the Serving Performance of Multi-LoRA Large Language Models via Efficient LoRA and KV Cache Management - arXiv, дата последнего обращения: декабря 13, 2025, https://arxiv.org/html/2505.03756v1
28. Support hot-swapping for LoRA adapters · Issue #9548 - GitHub, дата последнего обращения: декабря 13, 2025, https://github.com/ollama/ollama/issues/9548
29. support multiple lora adapters · Issue #7627 - GitHub, дата последнего обращения: декабря 13, 2025, https://github.com/ollama/ollama/issues/7627
30. Qwen2.5-1M: Deploy your own Qwen with context length up to 1M tokens | Hacker News, дата последнего обращения: декабря 13, 2025, https://news.ycombinator.com/item?id=42831769
31. KV cache strategies - Hugging Face, дата последнего обращения: декабря 13, 2025, https://huggingface.co/docs/transformers/kv_cache